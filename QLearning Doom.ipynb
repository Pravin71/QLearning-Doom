{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf             #Deep Learning library\n",
    "import numpy as np                  #Handle matrices\n",
    "from vizdoom import *               #Doom Enviroment\n",
    "\n",
    "from skimage import transform       #Helps to preprocess the frames\n",
    "import time                         #Handles time calculationsfundood\n",
    "\n",
    "import matplotlib.pyplot as plt     #Display graphs\n",
    "\n",
    "from collections import deque       #Ordered collection with ends\n",
    "\n",
    "import random\n",
    "import warnings                     #This will ignore all the warning messages that are normally printed during the training,\n",
    "                                    #caused by skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the Enviroment\n",
    "def create_enviroment():\n",
    "    game = DoomGame()\n",
    "\n",
    "    #Load the correct configuration \n",
    "    game.load_config(\"/mnt/fbc0d8ef-f929-44c3-b2ee-3c536d9e0645/QLearning/scenarios/basic.cfg\")\n",
    "    #load the right scenario(in this case the basic scenario)\n",
    "    game.set_doom_scenario_path(\"/mnt/fbc0d8ef-f929-44c3-b2ee-3c536d9e0645/QLearning/scenarios/basic.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "\n",
    "\"\"\"\n",
    "Use Random Actions to Test the Enviroment\n",
    "\"\"\"\n",
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"/mnt/fbc0d8ef-f929-44c3-b2ee-3c536d9e0645/QLearning/scenarios/basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"/mnt/fbc0d8ef-f929-44c3-b2ee-3c536d9e0645/QLearning/scenarios/basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    shoot = [0, 0, 1]\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    actions = [shoot, left, right]\n",
    "\n",
    "    episodes = 10\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            print (\"\\treward:\", reward)\n",
    "            time.sleep(0.02)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Enviroment and attain a list of possible actions to take\n",
    "game, possible_actions = create_enviroment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Function\n",
    "#The frame is preprocessed to reduce the complexity of our states and to reduce the computation time needed to complete\n",
    "#the training\n",
    "def preprocess_frame(frame):\n",
    "    #Greyscale frame already taken care of vizdoom config\n",
    "    \n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    #[Up: Down, Left: Right]---->30 down from the top, -10 up from the bottom , 30 from the left ,-30 from the right\n",
    "    cropped_frame = frame[30:-10, 30:-30]\n",
    "    \n",
    "    #Normalize pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    #Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "    \n",
    "    return preprocessed_frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At each time-step we stack every fourth frame\n",
    "#The frame is first preprocessed and the appended to the double ended queue deque\n",
    "#Every time a frame is appended to the deque it removes the oldest frame from the deque\n",
    "#Finally we build the stacked state\n",
    "stack_size = 4        #Stack 4 frames\n",
    "\n",
    "#Initialize deque with zero-images one array for each image\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype=np.int) for i in range(stack_size)], maxlen = 4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    #Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        #Clear stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84, 84), dtype=np.int) for i in range(stack_size)], maxlen = 4)\n",
    "        \n",
    "        #We are in a new episode, append the same frame 4 times\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        #Stack the frames across axis 2(depth 4) --> Join a sequence of arrays along new axis\n",
    "        stacked_state = np.stack(stacked_frames, axis = 2)\n",
    "    else:\n",
    "        #Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        \"\"\"\n",
    "        Build the stacked state --> Generates a tensor because stacked_frames is currently \n",
    "        just a double ended queue.\n",
    "        \"\"\"\n",
    "        stacked_state = np.stack(stacked_frames, axis = 2)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "#### Model Hyperparameters\n",
    "state_size = [84, 84, 4]            #Out input is a stack of 4 frames hence 84x84x4 (Width, height, channels)\n",
    "action_size = game.get_available_buttons_size()     #3 possbible actions\n",
    "learning_rate = 0.00025              #Alpha (learning rate)\n",
    "\n",
    "#### Training Hyperparameters\n",
    "total_episodes = 1000                #Total episodes for training\n",
    "max_steps = 200                      #Max possible steps in an episode\n",
    "batch_size = 64                      #Batch size\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0                  #Exploration probability at start\n",
    "explore_stop = 0.01                  #Minimum exploration probability\n",
    "decay_rate = 0.0001                  #Exponential decay rate for exploration probabilty\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.90                         #Discounting rate\n",
    "\n",
    "### Memory hyperparameters\n",
    "pretrain_length = batch_size         #Number of experiences stored in the Memory when initialized\n",
    "memory_size = 500000\n",
    "\n",
    "### Modify this to FALSE if you just want to see the Trained Agnet\n",
    "training = True\n",
    "\n",
    "### Turn this to TRUE if you want to render the enviorment\n",
    "episode_render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create The Deep Q-Learning Neural Network Model\n",
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name ='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        #Relates to variable sharing\n",
    "        with tf.variable_scope(name):\n",
    "            # We create the placeholders\n",
    "            # state_size = [84,84,4]   ------ *state_size = 84,84,4 basically removes brackets\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name = \"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name = \"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s',a')\n",
    "            #Maximum possible Qvalue for the next_state = Q_target\n",
    "            #Estimation since we can't have real Q value\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First Convolutional Layer\n",
    "            CNN\n",
    "            ELU ---> Activation fuction that provides better results for a CNN\n",
    "            \"\"\"\n",
    "            # Input is 84X84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_, \n",
    "                                          filters = 32, kernel_size = [8,8], strides = [4,4], padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv1\")\n",
    "            #Batch Normalization \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1, training = True, \n",
    "                                                                 epsilon = 1e-5, name = 'batch_norm1')\n",
    "            #ELU activation function\n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name = \"conv1_out\")\n",
    "            ## ----> [20, 20, 32]\n",
    "            \n",
    "            \"\"\"\n",
    "            Secound Convolutional Layer\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out, \n",
    "                                          filters = 64, kernel_size = [4,4], strides = [2,2], padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv2\")\n",
    "            #Batch Normalization \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2, training = True, \n",
    "                                                                 epsilon = 1e-5, name = 'batch_norm2')\n",
    "            #ELU activation function\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            \"\"\"\n",
    "            Third Convolutional Layer\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out, \n",
    "                                          filters = 64, kernel_size = [2,2], strides = [1,1], padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv3\")\n",
    "            #Batch Normalization \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3, training = True, \n",
    "                                                                 epsilon = 1e-5, name = 'batch_norm3')\n",
    "            \n",
    "            #ELU activation function\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            \"\"\"\n",
    "            Flatten\n",
    "            \"\"\"\n",
    "            \n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            \"\"\"\n",
    "            Fullyconnected Layers\n",
    "            512 units -----> action size(8 possible actions)\n",
    "            \"\"\"\n",
    "            self.fc = tf.layers.dense(inputs = self.flatten, units = 512, \n",
    "                                     activation = tf.nn.elu, \n",
    "                                     kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                     name = \"fc1\")\n",
    "            \n",
    "            #Outputs Q value for each Action\n",
    "            self.output = tf.layers.dense(inputs = self.fc , units = 3, \n",
    "                                     activation = None, \n",
    "                                     kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                     name = \"output\")\n",
    "            \n",
    "            \n",
    "            # Q is our predicted Q value\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis = 1)\n",
    "            \n",
    "            \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experience Replay\n",
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)     #Appends and experience to the expierence buffer\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)     #Attains length of buffer\n",
    "        #Generates a random sample from a 1D array\n",
    "        index = np.random.choice(np.arange(buffer_size), size = batch_size, replace = False)\n",
    "        return [self.buffer[i] for i in index] #Sample of expirences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill empty memory by taking random actions and storing state, next_state, reward, and action\n",
    "# Instantiate the Memory \n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "#pretrain_length = batch_size = 64 --> Number of expirences in memory when initialized\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "       # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        #Is a new episode so stack the same state 4 times\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    #If the episode is finished(we're dead 3x)\n",
    "    if done:\n",
    "        # We finsihed the episode so we make the last state a zero array\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory -- > state and new state(zero array)\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        #Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        #Attain State\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        #Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        #Attain State\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "            \n",
    "        # Our new state is now the next_state\n",
    "        state = next_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Tensorboard Writer\n",
    "writer = tf.summary.FileWriter(\"./tensorboard/dqn/1\")\n",
    "\n",
    "## Loesses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the Agent\n",
    "1.Sampling process --> sample the enviorment where we preform an action and store the observed expirences into\n",
    "  the replay memory\n",
    "2.Training select a small batch of random expirences and use it to preform a gradient descent update\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ##EPSLION GREEDY STRATEGY\n",
    "    #Choose an action from state S using epsilon greedy.\n",
    "    ##First choose random number from 0 to 1\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we will use the epsilon greedy strategy--> making use of expoential decay to reduce the epsilon value\n",
    "    over time. Epislon value will start of as 1 and slowly reduce. As the epsilon value reduces the algorithum \n",
    "    will explore less and exploit its attained expirence to gain rewards\n",
    "    \"\"\"\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    #Explore\n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        #Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "    #Exploit knowledge gained from past expireances\n",
    "    else:\n",
    "        # Get action from Q-network (exploitation) ---> leverage past expierence \n",
    "        # Estimate the Qs values state\n",
    "        # Note 1 in reshape coresponds to batch column in network\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        #Take the biggest Q value which is the best action --Estimate of largest future reward\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "        \n",
    "    return action, explore_probability\n",
    "\n",
    "#The saver will save our model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: -27.0 Explore P: 0.9899 Training Loss 165.7057\n",
      "Model Saved\n",
      "Model Saved\n",
      "Episode: 7 Total reward: -5.0 Explore P: 0.8716 Training Loss 0.3430\n",
      "Episode: 8 Total reward: 9.0 Explore P: 0.8650 Training Loss 40.2349\n",
      "Model Saved\n",
      "Episode: 13 Total reward: 95.0 Explore P: 0.7988 Training Loss 1.3169\n",
      "Episode: 14 Total reward: 73.0 Explore P: 0.7970 Training Loss 0.5638\n",
      "Model Saved\n",
      "Episode: 16 Total reward: 91.0 Explore P: 0.7806 Training Loss 6.4960\n",
      "Episode: 18 Total reward: 94.0 Explore P: 0.7648 Training Loss 7.5944\n",
      "Episode: 20 Total reward: 95.0 Explore P: 0.7494 Training Loss 7.0559\n",
      "Model Saved\n",
      "Episode: 23 Total reward: 92.0 Explore P: 0.7198 Training Loss 0.7300\n",
      "Episode: 25 Total reward: 31.0 Explore P: 0.7019 Training Loss 0.4396\n",
      "Model Saved\n",
      "Episode: 27 Total reward: -1.0 Explore P: 0.6824 Training Loss 5.2571\n",
      "Episode: 28 Total reward: 23.0 Explore P: 0.6781 Training Loss 2.2497\n",
      "Episode: 30 Total reward: 93.0 Explore P: 0.6644 Training Loss 1.6465\n",
      "Model Saved\n",
      "Episode: 31 Total reward: 23.0 Explore P: 0.6603 Training Loss 7.9270\n",
      "Episode: 33 Total reward: -2.0 Explore P: 0.6421 Training Loss 3.0156\n",
      "Episode: 35 Total reward: 32.0 Explore P: 0.6260 Training Loss 1.1964\n",
      "Model Saved\n",
      "Episode: 37 Total reward: -12.0 Explore P: 0.6082 Training Loss 1.1908\n",
      "Episode: 38 Total reward: 12.0 Explore P: 0.6041 Training Loss 0.6218\n",
      "Episode: 39 Total reward: 94.0 Explore P: 0.6037 Training Loss 1.3189\n",
      "Model Saved\n",
      "Episode: 41 Total reward: -141.0 Explore P: 0.5806 Training Loss 2.0292\n",
      "Episode: 42 Total reward: 67.0 Explore P: 0.5789 Training Loss 1.4652\n",
      "Episode: 45 Total reward: 95.0 Explore P: 0.5563 Training Loss 3.5427\n",
      "Model Saved\n",
      "Episode: 46 Total reward: 67.0 Explore P: 0.5547 Training Loss 1.3322\n",
      "Episode: 47 Total reward: -65.0 Explore P: 0.5473 Training Loss 2.5608\n",
      "Episode: 50 Total reward: 69.0 Explore P: 0.5249 Training Loss 0.9993\n",
      "Model Saved\n",
      "Episode: 51 Total reward: 63.0 Explore P: 0.5232 Training Loss 0.6732\n",
      "Episode: 53 Total reward: 87.0 Explore P: 0.5123 Training Loss 4.0047\n",
      "Episode: 54 Total reward: -46.0 Explore P: 0.5062 Training Loss 1.3775\n",
      "Episode: 55 Total reward: 94.0 Explore P: 0.5059 Training Loss 1.1894\n",
      "Model Saved\n",
      "Episode: 57 Total reward: 65.0 Explore P: 0.4945 Training Loss 1.1539\n",
      "Episode: 58 Total reward: 7.0 Explore P: 0.4907 Training Loss 1.8084\n",
      "Episode: 59 Total reward: 73.0 Explore P: 0.4896 Training Loss 1.6023\n",
      "Episode: 60 Total reward: 34.0 Explore P: 0.4869 Training Loss 1.1883\n",
      "Model Saved\n",
      "Episode: 61 Total reward: 63.0 Explore P: 0.4853 Training Loss 1.9873\n",
      "Episode: 62 Total reward: 95.0 Explore P: 0.4850 Training Loss 1.3750\n",
      "Episode: 63 Total reward: 91.0 Explore P: 0.4846 Training Loss 4.9702\n",
      "Episode: 64 Total reward: 94.0 Explore P: 0.4842 Training Loss 0.5339\n",
      "Model Saved\n",
      "Episode: 66 Total reward: 89.0 Explore P: 0.4743 Training Loss 2.4439\n",
      "Episode: 67 Total reward: 86.0 Explore P: 0.4736 Training Loss 1.7667\n",
      "Episode: 70 Total reward: 60.0 Explore P: 0.4538 Training Loss 2.9584\n",
      "Model Saved\n",
      "Episode: 71 Total reward: 69.0 Explore P: 0.4526 Training Loss 2.4841\n",
      "Episode: 72 Total reward: -124.0 Explore P: 0.4445 Training Loss 2.2637\n",
      "Episode: 73 Total reward: 47.0 Explore P: 0.4426 Training Loss 3.6071\n",
      "Episode: 74 Total reward: -10.0 Explore P: 0.4387 Training Loss 1.6819\n",
      "Episode: 75 Total reward: -51.0 Explore P: 0.4335 Training Loss 5.0656\n",
      "Model Saved\n",
      "Episode: 76 Total reward: -13.0 Explore P: 0.4295 Training Loss 3.1190\n",
      "Episode: 77 Total reward: 67.0 Explore P: 0.4283 Training Loss 3.5767\n",
      "Episode: 78 Total reward: 87.0 Explore P: 0.4277 Training Loss 4.4981\n",
      "Episode: 79 Total reward: 94.0 Explore P: 0.4274 Training Loss 5.7486\n",
      "Episode: 80 Total reward: 15.0 Explore P: 0.4245 Training Loss 6.4352\n",
      "Model Saved\n",
      "Episode: 81 Total reward: 94.0 Explore P: 0.4242 Training Loss 6.1541\n",
      "Episode: 85 Total reward: 95.0 Explore P: 0.3998 Training Loss 1.5014\n",
      "Model Saved\n",
      "Episode: 86 Total reward: 94.0 Explore P: 0.3996 Training Loss 6.0130\n",
      "Episode: 88 Total reward: 51.0 Explore P: 0.3903 Training Loss 7.1281\n",
      "Episode: 90 Total reward: 66.0 Explore P: 0.3817 Training Loss 1.1721\n",
      "Model Saved\n",
      "Episode: 91 Total reward: 95.0 Explore P: 0.3814 Training Loss 0.7687\n",
      "Episode: 92 Total reward: 25.0 Explore P: 0.3792 Training Loss 7.3851\n",
      "Episode: 93 Total reward: 93.0 Explore P: 0.3789 Training Loss 3.4291\n",
      "Episode: 94 Total reward: 46.0 Explore P: 0.3772 Training Loss 4.7207\n",
      "Episode: 95 Total reward: 35.0 Explore P: 0.3752 Training Loss 4.5933\n",
      "Model Saved\n",
      "Episode: 96 Total reward: 91.0 Explore P: 0.3748 Training Loss 7.0403\n",
      "Episode: 97 Total reward: 28.0 Explore P: 0.3725 Training Loss 5.6512\n",
      "Episode: 98 Total reward: 57.0 Explore P: 0.3711 Training Loss 1.7525\n",
      "Episode: 99 Total reward: 47.0 Explore P: 0.3695 Training Loss 2.4326\n",
      "Episode: 100 Total reward: 65.0 Explore P: 0.3684 Training Loss 1.3319\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 73.0 Explore P: 0.3676 Training Loss 6.8134\n",
      "Episode: 102 Total reward: 55.0 Explore P: 0.3661 Training Loss 2.5830\n",
      "Episode: 104 Total reward: 54.0 Explore P: 0.3576 Training Loss 4.9601\n",
      "Episode: 105 Total reward: 74.0 Explore P: 0.3567 Training Loss 5.2057\n",
      "Model Saved\n",
      "Episode: 107 Total reward: 55.0 Explore P: 0.3486 Training Loss 3.3289\n",
      "Episode: 108 Total reward: 41.0 Explore P: 0.3469 Training Loss 14.3451\n",
      "Episode: 109 Total reward: 59.0 Explore P: 0.3457 Training Loss 2.7037\n",
      "Episode: 110 Total reward: 87.0 Explore P: 0.3452 Training Loss 2.7410\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 95.0 Explore P: 0.3450 Training Loss 1.5308\n",
      "Episode: 112 Total reward: 47.0 Explore P: 0.3435 Training Loss 2.7633\n",
      "Episode: 113 Total reward: 50.0 Explore P: 0.3422 Training Loss 3.4615\n",
      "Episode: 114 Total reward: 76.0 Explore P: 0.3413 Training Loss 3.1089\n",
      "Episode: 115 Total reward: 46.0 Explore P: 0.3398 Training Loss 1.5324\n",
      "Model Saved\n",
      "Episode: 116 Total reward: -1.0 Explore P: 0.3370 Training Loss 10.2030\n",
      "Episode: 117 Total reward: 66.0 Explore P: 0.3360 Training Loss 11.4617\n",
      "Episode: 118 Total reward: 32.0 Explore P: 0.3341 Training Loss 2.1384\n",
      "Episode: 119 Total reward: 87.0 Explore P: 0.3336 Training Loss 6.0722\n",
      "Episode: 120 Total reward: 93.0 Explore P: 0.3334 Training Loss 1.5403\n",
      "Model Saved\n",
      "Episode: 121 Total reward: 95.0 Explore P: 0.3332 Training Loss 1.1188\n",
      "Episode: 122 Total reward: 34.0 Explore P: 0.3313 Training Loss 2.6344\n",
      "Episode: 123 Total reward: 18.0 Explore P: 0.3290 Training Loss 6.0871\n",
      "Episode: 124 Total reward: -108.0 Explore P: 0.3235 Training Loss 11.4634\n",
      "Episode: 125 Total reward: 65.0 Explore P: 0.3225 Training Loss 4.9775\n",
      "Model Saved\n",
      "Episode: 127 Total reward: 20.0 Explore P: 0.3143 Training Loss 3.3516\n",
      "Episode: 128 Total reward: 75.0 Explore P: 0.3137 Training Loss 5.0612\n",
      "Episode: 129 Total reward: 63.0 Explore P: 0.3127 Training Loss 2.5230\n",
      "Episode: 130 Total reward: 91.0 Explore P: 0.3124 Training Loss 5.0971\n",
      "Model Saved\n",
      "Episode: 131 Total reward: 45.0 Explore P: 0.3110 Training Loss 2.2939\n",
      "Episode: 132 Total reward: 15.0 Explore P: 0.3089 Training Loss 5.0883\n",
      "Episode: 133 Total reward: 32.0 Explore P: 0.3071 Training Loss 4.9800\n",
      "Episode: 134 Total reward: 95.0 Explore P: 0.3069 Training Loss 5.0294\n",
      "Episode: 135 Total reward: 95.0 Explore P: 0.3068 Training Loss 5.3325\n",
      "Model Saved\n",
      "Episode: 137 Total reward: 56.0 Explore P: 0.2997 Training Loss 3.8108\n",
      "Episode: 138 Total reward: 13.0 Explore P: 0.2976 Training Loss 10.4685\n",
      "Episode: 139 Total reward: 1.0 Explore P: 0.2952 Training Loss 3.6007\n",
      "Model Saved\n",
      "Episode: 142 Total reward: 95.0 Explore P: 0.2838 Training Loss 1.4988\n",
      "Episode: 143 Total reward: 71.0 Explore P: 0.2831 Training Loss 3.4451\n",
      "Episode: 144 Total reward: 95.0 Explore P: 0.2830 Training Loss 1.4374\n",
      "Episode: 145 Total reward: 63.0 Explore P: 0.2821 Training Loss 2.8916\n",
      "Model Saved\n",
      "Episode: 146 Total reward: 92.0 Explore P: 0.2818 Training Loss 2.7326\n",
      "Episode: 147 Total reward: 73.0 Explore P: 0.2811 Training Loss 4.4207\n",
      "Episode: 148 Total reward: 68.0 Explore P: 0.2803 Training Loss 2.5803\n",
      "Episode: 149 Total reward: -10.0 Explore P: 0.2779 Training Loss 2.0620\n",
      "Episode: 150 Total reward: 25.0 Explore P: 0.2761 Training Loss 2.8169\n",
      "Model Saved\n",
      "Episode: 151 Total reward: 61.0 Explore P: 0.2752 Training Loss 2.5628\n",
      "Episode: 153 Total reward: 38.0 Explore P: 0.2686 Training Loss 1.1534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 154 Total reward: 37.0 Explore P: 0.2672 Training Loss 4.1679\n",
      "Episode: 155 Total reward: 71.0 Explore P: 0.2664 Training Loss 2.5131\n",
      "Model Saved\n",
      "Episode: 157 Total reward: 72.0 Explore P: 0.2607 Training Loss 2.4839\n",
      "Episode: 158 Total reward: 34.0 Explore P: 0.2593 Training Loss 1.7686\n",
      "Episode: 159 Total reward: 12.0 Explore P: 0.2574 Training Loss 2.9528\n",
      "Episode: 160 Total reward: 94.0 Explore P: 0.2573 Training Loss 5.6054\n",
      "Model Saved\n",
      "Episode: 161 Total reward: -20.0 Explore P: 0.2548 Training Loss 3.3066\n",
      "Episode: 162 Total reward: 49.0 Explore P: 0.2538 Training Loss 12.5380\n",
      "Episode: 163 Total reward: 30.0 Explore P: 0.2524 Training Loss 2.4161\n",
      "Episode: 164 Total reward: 57.0 Explore P: 0.2516 Training Loss 5.3780\n",
      "Episode: 165 Total reward: 63.0 Explore P: 0.2508 Training Loss 2.3877\n",
      "Model Saved\n",
      "Episode: 166 Total reward: 77.0 Explore P: 0.2502 Training Loss 2.2992\n",
      "Episode: 167 Total reward: 58.0 Explore P: 0.2493 Training Loss 6.0660\n",
      "Episode: 168 Total reward: 11.0 Explore P: 0.2476 Training Loss 5.6683\n",
      "Episode: 169 Total reward: 60.0 Explore P: 0.2468 Training Loss 3.2587\n",
      "Episode: 170 Total reward: 74.0 Explore P: 0.2463 Training Loss 2.9961\n",
      "Model Saved\n",
      "Episode: 171 Total reward: 11.0 Explore P: 0.2445 Training Loss 4.7235\n",
      "Episode: 172 Total reward: 61.0 Explore P: 0.2437 Training Loss 2.7056\n",
      "Episode: 174 Total reward: 59.0 Explore P: 0.2382 Training Loss 5.8225\n",
      "Model Saved\n",
      "Episode: 176 Total reward: 94.0 Explore P: 0.2335 Training Loss 6.2225\n",
      "Episode: 177 Total reward: 61.0 Explore P: 0.2327 Training Loss 5.4083\n",
      "Episode: 178 Total reward: 58.0 Explore P: 0.2319 Training Loss 1.8024\n",
      "Episode: 179 Total reward: 56.0 Explore P: 0.2310 Training Loss 6.8843\n",
      "Model Saved\n",
      "Episode: 181 Total reward: 90.0 Explore P: 0.2264 Training Loss 2.3957\n",
      "Episode: 182 Total reward: 65.0 Explore P: 0.2257 Training Loss 1.7072\n",
      "Episode: 184 Total reward: 66.0 Explore P: 0.2208 Training Loss 1.3149\n",
      "Episode: 185 Total reward: 94.0 Explore P: 0.2207 Training Loss 5.8939\n",
      "Model Saved\n",
      "Episode: 186 Total reward: 58.0 Explore P: 0.2199 Training Loss 3.0255\n",
      "Episode: 187 Total reward: -2.0 Explore P: 0.2180 Training Loss 4.7869\n",
      "Episode: 188 Total reward: 95.0 Explore P: 0.2179 Training Loss 2.0284\n",
      "Episode: 189 Total reward: 62.0 Explore P: 0.2172 Training Loss 1.3734\n",
      "Episode: 190 Total reward: 86.0 Explore P: 0.2169 Training Loss 1.5828\n",
      "Model Saved\n",
      "Episode: 191 Total reward: 35.0 Explore P: 0.2157 Training Loss 2.6394\n",
      "Episode: 192 Total reward: 92.0 Explore P: 0.2156 Training Loss 2.6602\n",
      "Episode: 193 Total reward: 55.0 Explore P: 0.2147 Training Loss 6.2360\n",
      "Episode: 194 Total reward: 93.0 Explore P: 0.2145 Training Loss 1.2315\n",
      "Episode: 195 Total reward: 88.0 Explore P: 0.2143 Training Loss 1.8889\n",
      "Model Saved\n",
      "Episode: 196 Total reward: 40.0 Explore P: 0.2132 Training Loss 6.3935\n",
      "Episode: 197 Total reward: 56.0 Explore P: 0.2124 Training Loss 3.6886\n",
      "Episode: 198 Total reward: 91.0 Explore P: 0.2122 Training Loss 8.5868\n",
      "Episode: 199 Total reward: 60.0 Explore P: 0.2115 Training Loss 2.0818\n",
      "Model Saved\n",
      "Episode: 201 Total reward: 64.0 Explore P: 0.2069 Training Loss 2.7735\n",
      "Episode: 202 Total reward: 95.0 Explore P: 0.2068 Training Loss 4.8263\n",
      "Episode: 204 Total reward: 57.0 Explore P: 0.2021 Training Loss 4.1368\n",
      "Episode: 205 Total reward: 86.0 Explore P: 0.2018 Training Loss 3.2038\n",
      "Model Saved\n",
      "Episode: 206 Total reward: 54.0 Explore P: 0.2010 Training Loss 5.5999\n",
      "Episode: 207 Total reward: 87.0 Explore P: 0.2008 Training Loss 2.1274\n",
      "Episode: 208 Total reward: 69.0 Explore P: 0.2002 Training Loss 8.7127\n",
      "Episode: 209 Total reward: 58.0 Explore P: 0.1995 Training Loss 1.0692\n",
      "Episode: 210 Total reward: 39.0 Explore P: 0.1985 Training Loss 5.7725\n",
      "Model Saved\n",
      "Episode: 212 Total reward: 61.0 Explore P: 0.1942 Training Loss 2.9889\n",
      "Episode: 213 Total reward: 74.0 Explore P: 0.1938 Training Loss 5.3968\n",
      "Episode: 214 Total reward: 92.0 Explore P: 0.1936 Training Loss 3.3197\n",
      "Episode: 215 Total reward: 84.0 Explore P: 0.1933 Training Loss 1.4482\n",
      "Model Saved\n",
      "Episode: 216 Total reward: 87.0 Explore P: 0.1930 Training Loss 3.7958\n",
      "Episode: 218 Total reward: 37.0 Explore P: 0.1884 Training Loss 4.7344\n",
      "Episode: 219 Total reward: 79.0 Explore P: 0.1880 Training Loss 4.0321\n",
      "Episode: 220 Total reward: 95.0 Explore P: 0.1879 Training Loss 2.3416\n",
      "Model Saved\n",
      "Episode: 221 Total reward: 50.0 Explore P: 0.1872 Training Loss 1.3153\n",
      "Episode: 223 Total reward: 61.0 Explore P: 0.1831 Training Loss 5.0347\n",
      "Episode: 225 Total reward: 95.0 Explore P: 0.1796 Training Loss 4.3450\n",
      "Model Saved\n",
      "Episode: 226 Total reward: 95.0 Explore P: 0.1795 Training Loss 4.3074\n",
      "Episode: 227 Total reward: 95.0 Explore P: 0.1794 Training Loss 2.6004\n",
      "Episode: 228 Total reward: 92.0 Explore P: 0.1792 Training Loss 2.0528\n",
      "Episode: 229 Total reward: 95.0 Explore P: 0.1791 Training Loss 0.9037\n",
      "Model Saved\n",
      "Episode: 231 Total reward: 63.0 Explore P: 0.1752 Training Loss 2.4317\n",
      "Episode: 232 Total reward: 93.0 Explore P: 0.1751 Training Loss 4.6268\n",
      "Episode: 234 Total reward: 67.0 Explore P: 0.1713 Training Loss 2.2179\n",
      "Episode: 235 Total reward: 88.0 Explore P: 0.1711 Training Loss 1.3725\n",
      "Model Saved\n",
      "Episode: 236 Total reward: 95.0 Explore P: 0.1710 Training Loss 3.2768\n",
      "Episode: 237 Total reward: 59.0 Explore P: 0.1704 Training Loss 1.0036\n",
      "Episode: 238 Total reward: 49.0 Explore P: 0.1698 Training Loss 1.3567\n",
      "Episode: 239 Total reward: 66.0 Explore P: 0.1693 Training Loss 1.7492\n",
      "Episode: 240 Total reward: 66.0 Explore P: 0.1688 Training Loss 3.0796\n",
      "Model Saved\n",
      "Episode: 241 Total reward: 49.0 Explore P: 0.1681 Training Loss 2.3531\n",
      "Episode: 242 Total reward: 95.0 Explore P: 0.1680 Training Loss 3.1935\n",
      "Episode: 243 Total reward: -1.0 Explore P: 0.1667 Training Loss 3.5690\n",
      "Episode: 244 Total reward: 39.0 Explore P: 0.1659 Training Loss 3.2821\n",
      "Episode: 245 Total reward: 41.0 Explore P: 0.1651 Training Loss 2.8861\n",
      "Model Saved\n",
      "Episode: 246 Total reward: -3.0 Explore P: 0.1637 Training Loss 2.1890\n",
      "Episode: 247 Total reward: 43.0 Explore P: 0.1630 Training Loss 2.5667\n",
      "Episode: 249 Total reward: 40.0 Explore P: 0.1592 Training Loss 2.6990\n",
      "Episode: 250 Total reward: 90.0 Explore P: 0.1590 Training Loss 3.3192\n",
      "Model Saved\n",
      "Episode: 251 Total reward: 67.0 Explore P: 0.1586 Training Loss 2.4460\n",
      "Episode: 252 Total reward: 52.0 Explore P: 0.1579 Training Loss 7.5973\n",
      "Episode: 253 Total reward: 61.0 Explore P: 0.1574 Training Loss 5.0144\n",
      "Episode: 254 Total reward: 61.0 Explore P: 0.1569 Training Loss 4.9555\n",
      "Episode: 255 Total reward: 61.0 Explore P: 0.1564 Training Loss 4.7059\n",
      "Model Saved\n",
      "Episode: 256 Total reward: 74.0 Explore P: 0.1561 Training Loss 1.4735\n",
      "Episode: 257 Total reward: 85.0 Explore P: 0.1558 Training Loss 6.4097\n",
      "Episode: 258 Total reward: 43.0 Explore P: 0.1551 Training Loss 5.4212\n",
      "Episode: 259 Total reward: 37.0 Explore P: 0.1544 Training Loss 4.3848\n",
      "Model Saved\n",
      "Episode: 261 Total reward: 66.0 Explore P: 0.1511 Training Loss 3.5557\n",
      "Episode: 262 Total reward: 61.0 Explore P: 0.1506 Training Loss 5.1424\n",
      "Episode: 263 Total reward: 86.0 Explore P: 0.1504 Training Loss 2.6872\n",
      "Episode: 264 Total reward: 95.0 Explore P: 0.1503 Training Loss 5.9179\n",
      "Episode: 265 Total reward: 42.0 Explore P: 0.1496 Training Loss 4.7140\n",
      "Model Saved\n",
      "Episode: 266 Total reward: 95.0 Explore P: 0.1495 Training Loss 3.6872\n",
      "Episode: 267 Total reward: 62.0 Explore P: 0.1490 Training Loss 3.4683\n",
      "Episode: 268 Total reward: 92.0 Explore P: 0.1489 Training Loss 2.8755\n",
      "Episode: 269 Total reward: 59.0 Explore P: 0.1484 Training Loss 11.8316\n",
      "Episode: 270 Total reward: 65.0 Explore P: 0.1480 Training Loss 2.3584\n",
      "Model Saved\n",
      "Episode: 271 Total reward: 87.0 Explore P: 0.1478 Training Loss 4.5029\n",
      "Episode: 272 Total reward: 69.0 Explore P: 0.1474 Training Loss 2.3118\n",
      "Episode: 273 Total reward: 41.0 Explore P: 0.1467 Training Loss 1.3524\n",
      "Episode: 274 Total reward: 93.0 Explore P: 0.1466 Training Loss 5.7314\n",
      "Episode: 275 Total reward: 65.0 Explore P: 0.1462 Training Loss 5.1443\n",
      "Model Saved\n",
      "Episode: 278 Total reward: 28.0 Explore P: 0.1400 Training Loss 3.2075\n",
      "Episode: 279 Total reward: 73.0 Explore P: 0.1397 Training Loss 2.1216\n",
      "Episode: 280 Total reward: 50.0 Explore P: 0.1392 Training Loss 2.6330\n",
      "Model Saved\n",
      "Episode: 281 Total reward: 87.0 Explore P: 0.1390 Training Loss 2.5169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 282 Total reward: 88.0 Explore P: 0.1389 Training Loss 3.3595\n",
      "Episode: 283 Total reward: 95.0 Explore P: 0.1388 Training Loss 3.2703\n",
      "Episode: 285 Total reward: 61.0 Explore P: 0.1358 Training Loss 1.5539\n",
      "Model Saved\n",
      "Episode: 286 Total reward: 92.0 Explore P: 0.1357 Training Loss 1.4624\n",
      "Episode: 287 Total reward: 88.0 Explore P: 0.1355 Training Loss 1.4891\n",
      "Episode: 288 Total reward: 72.0 Explore P: 0.1352 Training Loss 4.2779\n",
      "Episode: 290 Total reward: 49.0 Explore P: 0.1322 Training Loss 3.8268\n",
      "Model Saved\n",
      "Episode: 291 Total reward: 69.0 Explore P: 0.1319 Training Loss 4.7290\n",
      "Episode: 292 Total reward: 95.0 Explore P: 0.1318 Training Loss 2.0570\n",
      "Episode: 293 Total reward: 86.0 Explore P: 0.1316 Training Loss 4.1843\n",
      "Episode: 294 Total reward: 95.0 Explore P: 0.1316 Training Loss 1.8005\n",
      "Episode: 295 Total reward: 95.0 Explore P: 0.1315 Training Loss 4.0690\n",
      "Model Saved\n",
      "Episode: 297 Total reward: 60.0 Explore P: 0.1287 Training Loss 1.8859\n",
      "Episode: 298 Total reward: 63.0 Explore P: 0.1283 Training Loss 4.3737\n",
      "Episode: 299 Total reward: 59.0 Explore P: 0.1278 Training Loss 1.5672\n",
      "Episode: 300 Total reward: 95.0 Explore P: 0.1278 Training Loss 3.7471\n",
      "Model Saved\n",
      "Episode: 301 Total reward: 72.0 Explore P: 0.1274 Training Loss 5.7529\n",
      "Episode: 302 Total reward: 60.0 Explore P: 0.1270 Training Loss 1.4457\n",
      "Episode: 303 Total reward: 65.0 Explore P: 0.1266 Training Loss 2.6087\n",
      "Episode: 304 Total reward: 93.0 Explore P: 0.1265 Training Loss 1.2840\n",
      "Episode: 305 Total reward: 64.0 Explore P: 0.1262 Training Loss 3.7448\n",
      "Model Saved\n",
      "Episode: 306 Total reward: 60.0 Explore P: 0.1257 Training Loss 1.5389\n",
      "Episode: 307 Total reward: 95.0 Explore P: 0.1257 Training Loss 4.0795\n",
      "Episode: 308 Total reward: 61.0 Explore P: 0.1253 Training Loss 2.5826\n",
      "Episode: 309 Total reward: 58.0 Explore P: 0.1248 Training Loss 2.7682\n",
      "Episode: 310 Total reward: 58.0 Explore P: 0.1244 Training Loss 3.8996\n",
      "Model Saved\n",
      "Episode: 311 Total reward: 74.0 Explore P: 0.1241 Training Loss 2.4871\n",
      "Episode: 312 Total reward: 95.0 Explore P: 0.1241 Training Loss 3.1264\n",
      "Episode: 313 Total reward: 65.0 Explore P: 0.1237 Training Loss 1.5757\n",
      "Episode: 314 Total reward: 15.0 Explore P: 0.1229 Training Loss 3.4136\n",
      "Episode: 315 Total reward: 85.0 Explore P: 0.1227 Training Loss 2.2801\n",
      "Model Saved\n",
      "Episode: 316 Total reward: 77.0 Explore P: 0.1225 Training Loss 1.4462\n",
      "Episode: 317 Total reward: 38.0 Explore P: 0.1219 Training Loss 1.0946\n",
      "Episode: 318 Total reward: 56.0 Explore P: 0.1214 Training Loss 2.9369\n",
      "Episode: 319 Total reward: 63.0 Explore P: 0.1211 Training Loss 6.0819\n",
      "Episode: 320 Total reward: 87.0 Explore P: 0.1209 Training Loss 4.4777\n",
      "Model Saved\n",
      "Episode: 321 Total reward: -44.0 Explore P: 0.1196 Training Loss 2.0832\n",
      "Episode: 322 Total reward: 67.0 Explore P: 0.1193 Training Loss 4.0380\n",
      "Episode: 323 Total reward: 63.0 Explore P: 0.1189 Training Loss 3.0046\n",
      "Episode: 324 Total reward: 64.0 Explore P: 0.1186 Training Loss 5.5054\n",
      "Episode: 325 Total reward: 95.0 Explore P: 0.1185 Training Loss 1.5393\n",
      "Model Saved\n",
      "Episode: 326 Total reward: 73.0 Explore P: 0.1182 Training Loss 11.7878\n",
      "Episode: 327 Total reward: 61.0 Explore P: 0.1179 Training Loss 6.2930\n",
      "Episode: 328 Total reward: 91.0 Explore P: 0.1178 Training Loss 8.9005\n",
      "Episode: 329 Total reward: 69.0 Explore P: 0.1175 Training Loss 5.6064\n",
      "Episode: 330 Total reward: 93.0 Explore P: 0.1174 Training Loss 2.8653\n",
      "Model Saved\n",
      "Episode: 331 Total reward: 69.0 Explore P: 0.1171 Training Loss 8.2272\n",
      "Episode: 332 Total reward: 67.0 Explore P: 0.1168 Training Loss 2.5781\n",
      "Episode: 333 Total reward: 95.0 Explore P: 0.1167 Training Loss 8.6415\n",
      "Episode: 334 Total reward: 84.0 Explore P: 0.1165 Training Loss 3.2029\n",
      "Episode: 335 Total reward: 46.0 Explore P: 0.1161 Training Loss 1.4989\n",
      "Model Saved\n",
      "Episode: 336 Total reward: 54.0 Explore P: 0.1157 Training Loss 3.8557\n",
      "Episode: 337 Total reward: 59.0 Explore P: 0.1153 Training Loss 3.3709\n",
      "Episode: 338 Total reward: 95.0 Explore P: 0.1152 Training Loss 5.8848\n",
      "Episode: 339 Total reward: 68.0 Explore P: 0.1149 Training Loss 3.2477\n",
      "Episode: 340 Total reward: 86.0 Explore P: 0.1148 Training Loss 2.4438\n",
      "Model Saved\n",
      "Episode: 341 Total reward: 93.0 Explore P: 0.1147 Training Loss 4.3532\n",
      "Episode: 342 Total reward: 66.0 Explore P: 0.1144 Training Loss 6.4724\n",
      "Episode: 343 Total reward: 63.0 Explore P: 0.1140 Training Loss 2.5229\n",
      "Episode: 344 Total reward: 63.0 Explore P: 0.1137 Training Loss 6.4867\n",
      "Episode: 345 Total reward: 67.0 Explore P: 0.1134 Training Loss 3.5582\n",
      "Model Saved\n",
      "Episode: 346 Total reward: 57.0 Explore P: 0.1130 Training Loss 2.5564\n",
      "Episode: 347 Total reward: 93.0 Explore P: 0.1129 Training Loss 3.9204\n",
      "Episode: 348 Total reward: 33.0 Explore P: 0.1123 Training Loss 3.7112\n",
      "Episode: 349 Total reward: 86.0 Explore P: 0.1121 Training Loss 2.9604\n",
      "Episode: 350 Total reward: 67.0 Explore P: 0.1119 Training Loss 6.1345\n",
      "Model Saved\n",
      "Episode: 351 Total reward: 80.0 Explore P: 0.1116 Training Loss 7.3636\n",
      "Episode: 352 Total reward: 95.0 Explore P: 0.1116 Training Loss 2.5011\n",
      "Episode: 353 Total reward: 80.0 Explore P: 0.1114 Training Loss 4.1330\n",
      "Episode: 354 Total reward: 84.0 Explore P: 0.1112 Training Loss 4.7077\n",
      "Episode: 355 Total reward: 62.0 Explore P: 0.1108 Training Loss 3.6422\n",
      "Model Saved\n",
      "Episode: 356 Total reward: 84.0 Explore P: 0.1107 Training Loss 3.9120\n",
      "Episode: 357 Total reward: 95.0 Explore P: 0.1106 Training Loss 1.3376\n",
      "Episode: 358 Total reward: 66.0 Explore P: 0.1103 Training Loss 6.2705\n",
      "Episode: 359 Total reward: 65.0 Explore P: 0.1100 Training Loss 2.8184\n",
      "Episode: 360 Total reward: 90.0 Explore P: 0.1099 Training Loss 8.5337\n",
      "Model Saved\n",
      "Episode: 361 Total reward: 61.0 Explore P: 0.1095 Training Loss 3.0571\n",
      "Episode: 362 Total reward: 94.0 Explore P: 0.1095 Training Loss 4.6153\n",
      "Episode: 363 Total reward: 48.0 Explore P: 0.1090 Training Loss 2.2088\n",
      "Episode: 365 Total reward: 52.0 Explore P: 0.1067 Training Loss 1.9803\n",
      "Model Saved\n",
      "Episode: 366 Total reward: 95.0 Explore P: 0.1067 Training Loss 2.2617\n",
      "Episode: 367 Total reward: 88.0 Explore P: 0.1065 Training Loss 9.7516\n",
      "Episode: 368 Total reward: 54.0 Explore P: 0.1061 Training Loss 8.2376\n",
      "Episode: 369 Total reward: 95.0 Explore P: 0.1061 Training Loss 5.0265\n",
      "Episode: 370 Total reward: 87.0 Explore P: 0.1059 Training Loss 6.7357\n",
      "Model Saved\n",
      "Episode: 371 Total reward: 55.0 Explore P: 0.1055 Training Loss 4.1325\n",
      "Episode: 372 Total reward: 79.0 Explore P: 0.1053 Training Loss 4.5976\n",
      "Episode: 373 Total reward: 43.0 Explore P: 0.1049 Training Loss 3.1981\n",
      "Episode: 374 Total reward: 59.0 Explore P: 0.1045 Training Loss 3.5814\n",
      "Episode: 375 Total reward: 60.0 Explore P: 0.1042 Training Loss 4.2174\n",
      "Model Saved\n",
      "Episode: 376 Total reward: 95.0 Explore P: 0.1041 Training Loss 3.8740\n",
      "Episode: 377 Total reward: 56.0 Explore P: 0.1037 Training Loss 5.8413\n",
      "Episode: 378 Total reward: 60.0 Explore P: 0.1034 Training Loss 3.1502\n",
      "Episode: 379 Total reward: 62.0 Explore P: 0.1031 Training Loss 2.2545\n",
      "Episode: 380 Total reward: 64.0 Explore P: 0.1028 Training Loss 1.2490\n",
      "Model Saved\n",
      "Episode: 381 Total reward: 60.0 Explore P: 0.1025 Training Loss 7.7563\n",
      "Episode: 382 Total reward: 61.0 Explore P: 0.1021 Training Loss 4.1201\n",
      "Episode: 383 Total reward: 61.0 Explore P: 0.1018 Training Loss 1.9369\n",
      "Episode: 384 Total reward: 39.0 Explore P: 0.1013 Training Loss 5.5078\n",
      "Episode: 385 Total reward: 41.0 Explore P: 0.1008 Training Loss 4.1723\n",
      "Model Saved\n",
      "Episode: 386 Total reward: 91.0 Explore P: 0.1007 Training Loss 3.9609\n",
      "Episode: 387 Total reward: 95.0 Explore P: 0.1007 Training Loss 3.8021\n",
      "Episode: 388 Total reward: 65.0 Explore P: 0.1004 Training Loss 2.1922\n",
      "Episode: 389 Total reward: 63.0 Explore P: 0.1001 Training Loss 1.4431\n",
      "Episode: 390 Total reward: 84.0 Explore P: 0.1000 Training Loss 5.2997\n",
      "Model Saved\n",
      "Episode: 391 Total reward: 65.0 Explore P: 0.0997 Training Loss 3.1133\n",
      "Episode: 392 Total reward: 59.0 Explore P: 0.0994 Training Loss 3.5985\n",
      "Episode: 393 Total reward: 95.0 Explore P: 0.0993 Training Loss 2.1307\n",
      "Episode: 394 Total reward: 74.0 Explore P: 0.0991 Training Loss 1.9724\n",
      "Episode: 395 Total reward: 65.0 Explore P: 0.0988 Training Loss 3.2034\n",
      "Model Saved\n",
      "Episode: 396 Total reward: 58.0 Explore P: 0.0985 Training Loss 2.8874\n",
      "Episode: 397 Total reward: 86.0 Explore P: 0.0984 Training Loss 2.5105\n",
      "Episode: 398 Total reward: 94.0 Explore P: 0.0983 Training Loss 1.4230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 399 Total reward: 79.0 Explore P: 0.0981 Training Loss 3.2681\n",
      "Episode: 400 Total reward: 68.0 Explore P: 0.0979 Training Loss 6.8805\n",
      "Model Saved\n",
      "Episode: 401 Total reward: 86.0 Explore P: 0.0977 Training Loss 3.7146\n",
      "Episode: 402 Total reward: 67.0 Explore P: 0.0975 Training Loss 1.7672\n",
      "Episode: 403 Total reward: 54.0 Explore P: 0.0971 Training Loss 4.5166\n",
      "Episode: 404 Total reward: 60.0 Explore P: 0.0968 Training Loss 1.4448\n",
      "Episode: 405 Total reward: 64.0 Explore P: 0.0965 Training Loss 7.8011\n",
      "Model Saved\n",
      "Episode: 406 Total reward: 89.0 Explore P: 0.0964 Training Loss 3.5303\n",
      "Episode: 407 Total reward: 58.0 Explore P: 0.0961 Training Loss 4.7844\n",
      "Episode: 408 Total reward: 76.0 Explore P: 0.0959 Training Loss 9.8946\n",
      "Episode: 409 Total reward: 76.0 Explore P: 0.0957 Training Loss 4.0694\n",
      "Episode: 410 Total reward: 80.0 Explore P: 0.0956 Training Loss 5.5422\n",
      "Model Saved\n",
      "Episode: 411 Total reward: 77.0 Explore P: 0.0954 Training Loss 7.4817\n",
      "Episode: 412 Total reward: 85.0 Explore P: 0.0952 Training Loss 4.6235\n",
      "Episode: 413 Total reward: 64.0 Explore P: 0.0949 Training Loss 5.2579\n",
      "Episode: 414 Total reward: 93.0 Explore P: 0.0949 Training Loss 3.5747\n",
      "Episode: 415 Total reward: 65.0 Explore P: 0.0946 Training Loss 2.9344\n",
      "Model Saved\n",
      "Episode: 416 Total reward: 60.0 Explore P: 0.0943 Training Loss 4.1225\n",
      "Episode: 417 Total reward: 79.0 Explore P: 0.0941 Training Loss 4.8425\n",
      "Episode: 418 Total reward: 89.0 Explore P: 0.0940 Training Loss 13.1701\n",
      "Episode: 419 Total reward: 48.0 Explore P: 0.0937 Training Loss 4.6031\n",
      "Episode: 420 Total reward: 95.0 Explore P: 0.0936 Training Loss 4.1485\n",
      "Model Saved\n",
      "Episode: 421 Total reward: 66.0 Explore P: 0.0934 Training Loss 2.9567\n",
      "Episode: 422 Total reward: 48.0 Explore P: 0.0930 Training Loss 6.3488\n",
      "Episode: 423 Total reward: 77.0 Explore P: 0.0928 Training Loss 1.2355\n",
      "Episode: 424 Total reward: 93.0 Explore P: 0.0927 Training Loss 3.3587\n",
      "Episode: 425 Total reward: 56.0 Explore P: 0.0924 Training Loss 3.0395\n",
      "Model Saved\n",
      "Episode: 426 Total reward: 79.0 Explore P: 0.0922 Training Loss 2.9775\n",
      "Episode: 427 Total reward: 84.0 Explore P: 0.0921 Training Loss 2.3112\n",
      "Episode: 428 Total reward: 95.0 Explore P: 0.0920 Training Loss 4.0437\n",
      "Episode: 429 Total reward: 91.0 Explore P: 0.0920 Training Loss 3.5320\n",
      "Episode: 430 Total reward: 87.0 Explore P: 0.0918 Training Loss 7.0355\n",
      "Model Saved\n",
      "Episode: 431 Total reward: 80.0 Explore P: 0.0917 Training Loss 7.2992\n",
      "Episode: 432 Total reward: 67.0 Explore P: 0.0914 Training Loss 5.2690\n",
      "Episode: 433 Total reward: 61.0 Explore P: 0.0911 Training Loss 3.2063\n",
      "Episode: 434 Total reward: 56.0 Explore P: 0.0908 Training Loss 8.0052\n",
      "Episode: 435 Total reward: 94.0 Explore P: 0.0907 Training Loss 4.8823\n",
      "Model Saved\n",
      "Episode: 436 Total reward: 51.0 Explore P: 0.0904 Training Loss 7.9848\n",
      "Episode: 437 Total reward: 57.0 Explore P: 0.0901 Training Loss 5.6853\n",
      "Episode: 438 Total reward: 95.0 Explore P: 0.0900 Training Loss 2.1692\n",
      "Episode: 439 Total reward: 81.0 Explore P: 0.0899 Training Loss 1.7828\n",
      "Episode: 440 Total reward: 95.0 Explore P: 0.0898 Training Loss 3.8671\n",
      "Model Saved\n",
      "Episode: 441 Total reward: 95.0 Explore P: 0.0898 Training Loss 5.2467\n",
      "Episode: 442 Total reward: 95.0 Explore P: 0.0897 Training Loss 1.4875\n",
      "Episode: 443 Total reward: 93.0 Explore P: 0.0897 Training Loss 4.3233\n",
      "Episode: 444 Total reward: 90.0 Explore P: 0.0896 Training Loss 2.8020\n",
      "Episode: 445 Total reward: 36.0 Explore P: 0.0892 Training Loss 3.5538\n",
      "Model Saved\n",
      "Episode: 446 Total reward: 57.0 Explore P: 0.0888 Training Loss 5.3995\n",
      "Episode: 447 Total reward: 76.0 Explore P: 0.0886 Training Loss 4.8418\n",
      "Episode: 448 Total reward: 60.0 Explore P: 0.0884 Training Loss 6.2753\n",
      "Episode: 450 Total reward: 62.0 Explore P: 0.0866 Training Loss 2.5298\n",
      "Model Saved\n",
      "Episode: 451 Total reward: 77.0 Explore P: 0.0864 Training Loss 2.1643\n",
      "Episode: 452 Total reward: 53.0 Explore P: 0.0860 Training Loss 2.1054\n",
      "Episode: 453 Total reward: 61.0 Explore P: 0.0858 Training Loss 3.9481\n",
      "Episode: 454 Total reward: 57.0 Explore P: 0.0855 Training Loss 5.8571\n",
      "Episode: 455 Total reward: 58.0 Explore P: 0.0852 Training Loss 5.8229\n",
      "Model Saved\n",
      "Episode: 456 Total reward: 57.0 Explore P: 0.0849 Training Loss 4.8319\n",
      "Episode: 457 Total reward: 92.0 Explore P: 0.0849 Training Loss 2.0008\n",
      "Episode: 458 Total reward: 38.0 Explore P: 0.0845 Training Loss 6.0864\n",
      "Episode: 459 Total reward: 56.0 Explore P: 0.0842 Training Loss 5.6813\n",
      "Episode: 460 Total reward: 58.0 Explore P: 0.0839 Training Loss 4.4948\n",
      "Model Saved\n",
      "Episode: 461 Total reward: 64.0 Explore P: 0.0837 Training Loss 52.4620\n",
      "Episode: 462 Total reward: 65.0 Explore P: 0.0834 Training Loss 6.5162\n",
      "Episode: 463 Total reward: 93.0 Explore P: 0.0834 Training Loss 4.1300\n",
      "Episode: 464 Total reward: 77.0 Explore P: 0.0832 Training Loss 5.3021\n",
      "Episode: 465 Total reward: 66.0 Explore P: 0.0830 Training Loss 5.2498\n",
      "Model Saved\n",
      "Episode: 466 Total reward: 49.0 Explore P: 0.0827 Training Loss 11.2687\n",
      "Episode: 467 Total reward: 55.0 Explore P: 0.0824 Training Loss 3.3032\n",
      "Episode: 468 Total reward: 61.0 Explore P: 0.0821 Training Loss 1.4182\n",
      "Episode: 469 Total reward: 95.0 Explore P: 0.0821 Training Loss 3.5030\n",
      "Episode: 470 Total reward: 74.0 Explore P: 0.0819 Training Loss 1.5523\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 46.0 Explore P: 0.0816 Training Loss 3.5891\n",
      "Episode: 472 Total reward: 80.0 Explore P: 0.0815 Training Loss 2.9470\n",
      "Episode: 473 Total reward: 59.0 Explore P: 0.0812 Training Loss 11.7858\n",
      "Episode: 474 Total reward: 61.0 Explore P: 0.0809 Training Loss 8.2447\n",
      "Episode: 475 Total reward: 76.0 Explore P: 0.0808 Training Loss 6.0195\n",
      "Model Saved\n",
      "Episode: 477 Total reward: 58.0 Explore P: 0.0791 Training Loss 4.4784\n",
      "Episode: 478 Total reward: 93.0 Explore P: 0.0791 Training Loss 1.8960\n",
      "Episode: 479 Total reward: 95.0 Explore P: 0.0790 Training Loss 10.3174\n",
      "Episode: 480 Total reward: 67.0 Explore P: 0.0788 Training Loss 3.1766\n",
      "Model Saved\n",
      "Episode: 481 Total reward: 95.0 Explore P: 0.0788 Training Loss 3.0296\n",
      "Episode: 482 Total reward: 67.0 Explore P: 0.0786 Training Loss 6.6962\n",
      "Episode: 483 Total reward: 94.0 Explore P: 0.0785 Training Loss 3.7723\n",
      "Episode: 484 Total reward: 63.0 Explore P: 0.0783 Training Loss 2.9363\n",
      "Episode: 485 Total reward: 93.0 Explore P: 0.0783 Training Loss 1.9469\n",
      "Model Saved\n",
      "Episode: 486 Total reward: 87.0 Explore P: 0.0782 Training Loss 4.0226\n",
      "Episode: 487 Total reward: 95.0 Explore P: 0.0781 Training Loss 8.5654\n",
      "Episode: 488 Total reward: 66.0 Explore P: 0.0779 Training Loss 2.6056\n",
      "Episode: 489 Total reward: 88.0 Explore P: 0.0778 Training Loss 2.0631\n",
      "Episode: 490 Total reward: 94.0 Explore P: 0.0778 Training Loss 2.5243\n",
      "Model Saved\n",
      "Episode: 491 Total reward: 61.0 Explore P: 0.0776 Training Loss 2.4309\n",
      "Episode: 492 Total reward: 87.0 Explore P: 0.0775 Training Loss 2.4985\n",
      "Episode: 493 Total reward: 95.0 Explore P: 0.0774 Training Loss 7.8412\n",
      "Episode: 494 Total reward: 67.0 Explore P: 0.0772 Training Loss 4.3779\n",
      "Episode: 495 Total reward: 94.0 Explore P: 0.0772 Training Loss 3.6660\n",
      "Model Saved\n",
      "Episode: 496 Total reward: 60.0 Explore P: 0.0769 Training Loss 2.6963\n",
      "Episode: 497 Total reward: 85.0 Explore P: 0.0768 Training Loss 5.9799\n",
      "Episode: 498 Total reward: 60.0 Explore P: 0.0766 Training Loss 3.2075\n",
      "Episode: 499 Total reward: 66.0 Explore P: 0.0764 Training Loss 2.7727\n",
      "Episode: 500 Total reward: 95.0 Explore P: 0.0763 Training Loss 3.3762\n",
      "Model Saved\n",
      "Episode: 501 Total reward: 58.0 Explore P: 0.0761 Training Loss 2.3975\n",
      "Episode: 502 Total reward: 33.0 Explore P: 0.0757 Training Loss 8.0894\n",
      "Episode: 503 Total reward: 61.0 Explore P: 0.0755 Training Loss 3.1487\n",
      "Episode: 504 Total reward: 79.0 Explore P: 0.0753 Training Loss 2.3191\n",
      "Episode: 505 Total reward: 86.0 Explore P: 0.0752 Training Loss 4.3186\n",
      "Model Saved\n",
      "Episode: 506 Total reward: 87.0 Explore P: 0.0752 Training Loss 1.5431\n",
      "Episode: 507 Total reward: 61.0 Explore P: 0.0749 Training Loss 3.4642\n",
      "Episode: 508 Total reward: 61.0 Explore P: 0.0747 Training Loss 3.9684\n",
      "Episode: 509 Total reward: 63.0 Explore P: 0.0745 Training Loss 6.5116\n",
      "Episode: 510 Total reward: 81.0 Explore P: 0.0743 Training Loss 3.3953\n",
      "Model Saved\n",
      "Episode: 511 Total reward: 80.0 Explore P: 0.0742 Training Loss 2.9428\n",
      "Episode: 512 Total reward: 92.0 Explore P: 0.0741 Training Loss 2.0964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 513 Total reward: 80.0 Explore P: 0.0740 Training Loss 3.0969\n",
      "Episode: 514 Total reward: 74.0 Explore P: 0.0739 Training Loss 3.2449\n",
      "Episode: 515 Total reward: 58.0 Explore P: 0.0736 Training Loss 2.4500\n",
      "Model Saved\n",
      "Episode: 516 Total reward: 86.0 Explore P: 0.0735 Training Loss 4.0780\n",
      "Episode: 517 Total reward: 93.0 Explore P: 0.0735 Training Loss 6.0169\n",
      "Episode: 518 Total reward: 61.0 Explore P: 0.0732 Training Loss 2.1515\n",
      "Episode: 519 Total reward: 61.0 Explore P: 0.0730 Training Loss 6.7753\n",
      "Episode: 520 Total reward: 60.0 Explore P: 0.0728 Training Loss 2.5635\n",
      "Model Saved\n",
      "Episode: 521 Total reward: 69.0 Explore P: 0.0726 Training Loss 3.1743\n",
      "Episode: 522 Total reward: 95.0 Explore P: 0.0726 Training Loss 2.9943\n",
      "Episode: 523 Total reward: 94.0 Explore P: 0.0725 Training Loss 2.5272\n",
      "Episode: 524 Total reward: 94.0 Explore P: 0.0725 Training Loss 2.9811\n",
      "Episode: 525 Total reward: 48.0 Explore P: 0.0722 Training Loss 4.4569\n",
      "Model Saved\n",
      "Episode: 526 Total reward: 67.0 Explore P: 0.0721 Training Loss 10.9495\n",
      "Episode: 527 Total reward: 58.0 Explore P: 0.0718 Training Loss 9.1756\n",
      "Episode: 529 Total reward: 64.0 Explore P: 0.0704 Training Loss 3.9144\n",
      "Episode: 530 Total reward: 95.0 Explore P: 0.0704 Training Loss 1.6387\n",
      "Model Saved\n",
      "Episode: 531 Total reward: 95.0 Explore P: 0.0703 Training Loss 2.1722\n",
      "Episode: 532 Total reward: 77.0 Explore P: 0.0702 Training Loss 3.1548\n",
      "Episode: 533 Total reward: 86.0 Explore P: 0.0701 Training Loss 1.4628\n",
      "Episode: 534 Total reward: 61.0 Explore P: 0.0699 Training Loss 3.7249\n",
      "Episode: 535 Total reward: 94.0 Explore P: 0.0698 Training Loss 4.4171\n",
      "Model Saved\n",
      "Episode: 536 Total reward: 77.0 Explore P: 0.0697 Training Loss 7.1016\n",
      "Episode: 537 Total reward: 94.0 Explore P: 0.0697 Training Loss 3.7028\n",
      "Episode: 538 Total reward: 71.0 Explore P: 0.0695 Training Loss 5.6718\n",
      "Episode: 539 Total reward: 50.0 Explore P: 0.0693 Training Loss 4.3535\n",
      "Model Saved\n",
      "Episode: 541 Total reward: 71.0 Explore P: 0.0679 Training Loss 5.3359\n",
      "Episode: 542 Total reward: 52.0 Explore P: 0.0677 Training Loss 3.4531\n",
      "Episode: 543 Total reward: 94.0 Explore P: 0.0677 Training Loss 5.9492\n",
      "Episode: 544 Total reward: 84.0 Explore P: 0.0676 Training Loss 7.2844\n",
      "Episode: 545 Total reward: 95.0 Explore P: 0.0675 Training Loss 6.5094\n",
      "Model Saved\n",
      "Episode: 546 Total reward: 49.0 Explore P: 0.0673 Training Loss 3.5286\n",
      "Episode: 547 Total reward: 52.0 Explore P: 0.0671 Training Loss 2.5840\n",
      "Episode: 548 Total reward: 95.0 Explore P: 0.0670 Training Loss 1.6268\n",
      "Episode: 549 Total reward: 92.0 Explore P: 0.0670 Training Loss 7.0438\n",
      "Model Saved\n",
      "Episode: 551 Total reward: 90.0 Explore P: 0.0658 Training Loss 2.0246\n",
      "Episode: 552 Total reward: 83.0 Explore P: 0.0657 Training Loss 6.2400\n",
      "Episode: 553 Total reward: 58.0 Explore P: 0.0655 Training Loss 4.2065\n",
      "Episode: 554 Total reward: 64.0 Explore P: 0.0653 Training Loss 2.3389\n",
      "Episode: 555 Total reward: 79.0 Explore P: 0.0652 Training Loss 5.8508\n",
      "Model Saved\n",
      "Episode: 556 Total reward: 94.0 Explore P: 0.0651 Training Loss 1.0504\n",
      "Episode: 557 Total reward: 77.0 Explore P: 0.0650 Training Loss 4.6558\n",
      "Episode: 558 Total reward: 77.0 Explore P: 0.0649 Training Loss 4.7936\n",
      "Episode: 559 Total reward: 82.0 Explore P: 0.0648 Training Loss 2.5543\n",
      "Episode: 560 Total reward: 77.0 Explore P: 0.0646 Training Loss 3.5790\n",
      "Model Saved\n",
      "Episode: 561 Total reward: 46.0 Explore P: 0.0644 Training Loss 4.0274\n",
      "Episode: 563 Total reward: 62.0 Explore P: 0.0631 Training Loss 3.3797\n",
      "Episode: 564 Total reward: 87.0 Explore P: 0.0631 Training Loss 2.1318\n",
      "Episode: 565 Total reward: 61.0 Explore P: 0.0629 Training Loss 1.0401\n",
      "Model Saved\n",
      "Episode: 566 Total reward: 54.0 Explore P: 0.0626 Training Loss 4.9849\n",
      "Episode: 567 Total reward: 95.0 Explore P: 0.0626 Training Loss 2.7624\n",
      "Episode: 568 Total reward: 87.0 Explore P: 0.0625 Training Loss 2.4475\n",
      "Episode: 569 Total reward: 58.0 Explore P: 0.0623 Training Loss 7.0083\n",
      "Episode: 570 Total reward: 79.0 Explore P: 0.0622 Training Loss 4.3434\n",
      "Model Saved\n",
      "Episode: 571 Total reward: 60.0 Explore P: 0.0620 Training Loss 3.9746\n",
      "Episode: 572 Total reward: 77.0 Explore P: 0.0619 Training Loss 3.3507\n",
      "Episode: 573 Total reward: 60.0 Explore P: 0.0617 Training Loss 2.3597\n",
      "Episode: 574 Total reward: 60.0 Explore P: 0.0615 Training Loss 1.1453\n",
      "Episode: 575 Total reward: 64.0 Explore P: 0.0614 Training Loss 6.4300\n",
      "Model Saved\n",
      "Episode: 576 Total reward: 86.0 Explore P: 0.0613 Training Loss 3.4563\n",
      "Episode: 577 Total reward: 84.0 Explore P: 0.0612 Training Loss 3.7259\n",
      "Episode: 578 Total reward: 77.0 Explore P: 0.0611 Training Loss 8.0078\n",
      "Episode: 579 Total reward: 52.0 Explore P: 0.0609 Training Loss 5.7452\n",
      "Episode: 580 Total reward: 57.0 Explore P: 0.0607 Training Loss 5.0407\n",
      "Model Saved\n",
      "Episode: 581 Total reward: 85.0 Explore P: 0.0606 Training Loss 3.1857\n",
      "Episode: 582 Total reward: 95.0 Explore P: 0.0606 Training Loss 2.5340\n",
      "Episode: 583 Total reward: 64.0 Explore P: 0.0604 Training Loss 6.2227\n",
      "Episode: 584 Total reward: 79.0 Explore P: 0.0603 Training Loss 3.6484\n",
      "Episode: 585 Total reward: 60.0 Explore P: 0.0601 Training Loss 3.5296\n",
      "Model Saved\n",
      "Episode: 586 Total reward: 82.0 Explore P: 0.0600 Training Loss 4.8654\n",
      "Episode: 587 Total reward: 95.0 Explore P: 0.0600 Training Loss 3.0399\n",
      "Episode: 588 Total reward: 60.0 Explore P: 0.0598 Training Loss 5.5258\n",
      "Episode: 589 Total reward: 55.0 Explore P: 0.0596 Training Loss 2.7994\n",
      "Episode: 590 Total reward: 61.0 Explore P: 0.0594 Training Loss 3.2729\n",
      "Model Saved\n",
      "Episode: 591 Total reward: 72.0 Explore P: 0.0593 Training Loss 4.0162\n",
      "Episode: 592 Total reward: 61.0 Explore P: 0.0591 Training Loss 1.9894\n",
      "Episode: 593 Total reward: 95.0 Explore P: 0.0591 Training Loss 6.0898\n",
      "Episode: 595 Total reward: 58.0 Explore P: 0.0579 Training Loss 4.5705\n",
      "Model Saved\n",
      "Episode: 597 Total reward: 69.0 Explore P: 0.0569 Training Loss 1.6349\n",
      "Episode: 598 Total reward: 84.0 Explore P: 0.0568 Training Loss 1.6187\n",
      "Episode: 599 Total reward: 87.0 Explore P: 0.0567 Training Loss 4.5331\n",
      "Episode: 600 Total reward: 94.0 Explore P: 0.0567 Training Loss 4.3449\n",
      "Model Saved\n",
      "Episode: 601 Total reward: 68.0 Explore P: 0.0566 Training Loss 3.0762\n",
      "Episode: 602 Total reward: 80.0 Explore P: 0.0565 Training Loss 3.3402\n",
      "Episode: 603 Total reward: 94.0 Explore P: 0.0564 Training Loss 8.1874\n",
      "Episode: 604 Total reward: 81.0 Explore P: 0.0563 Training Loss 4.2065\n",
      "Episode: 605 Total reward: 58.0 Explore P: 0.0562 Training Loss 5.8279\n",
      "Model Saved\n",
      "Episode: 606 Total reward: 41.0 Explore P: 0.0559 Training Loss 2.2081\n",
      "Episode: 607 Total reward: 94.0 Explore P: 0.0559 Training Loss 4.1371\n",
      "Episode: 608 Total reward: 58.0 Explore P: 0.0557 Training Loss 2.8346\n",
      "Episode: 609 Total reward: 71.0 Explore P: 0.0556 Training Loss 2.4140\n",
      "Episode: 610 Total reward: 65.0 Explore P: 0.0555 Training Loss 2.5731\n",
      "Model Saved\n",
      "Episode: 611 Total reward: 86.0 Explore P: 0.0554 Training Loss 2.1693\n",
      "Episode: 612 Total reward: 70.0 Explore P: 0.0553 Training Loss 2.2127\n",
      "Episode: 613 Total reward: 88.0 Explore P: 0.0552 Training Loss 3.2913\n",
      "Episode: 614 Total reward: 80.0 Explore P: 0.0551 Training Loss 3.5465\n",
      "Episode: 615 Total reward: 49.0 Explore P: 0.0549 Training Loss 4.2522\n",
      "Model Saved\n",
      "Episode: 616 Total reward: 76.0 Explore P: 0.0548 Training Loss 3.4163\n",
      "Episode: 617 Total reward: 85.0 Explore P: 0.0548 Training Loss 2.0255\n",
      "Episode: 618 Total reward: 86.0 Explore P: 0.0547 Training Loss 2.2623\n",
      "Episode: 619 Total reward: 53.0 Explore P: 0.0545 Training Loss 3.5939\n",
      "Episode: 620 Total reward: 95.0 Explore P: 0.0545 Training Loss 5.7760\n",
      "Model Saved\n",
      "Episode: 622 Total reward: 61.0 Explore P: 0.0534 Training Loss 6.1502\n",
      "Episode: 623 Total reward: 77.0 Explore P: 0.0533 Training Loss 2.0708\n",
      "Episode: 624 Total reward: 60.0 Explore P: 0.0532 Training Loss 5.6656\n",
      "Episode: 625 Total reward: 61.0 Explore P: 0.0530 Training Loss 1.4892\n",
      "Model Saved\n",
      "Episode: 626 Total reward: 77.0 Explore P: 0.0529 Training Loss 3.1094\n",
      "Episode: 627 Total reward: 85.0 Explore P: 0.0529 Training Loss 3.7100\n",
      "Episode: 628 Total reward: 58.0 Explore P: 0.0527 Training Loss 2.2592\n",
      "Episode: 629 Total reward: 89.0 Explore P: 0.0526 Training Loss 2.2593\n",
      "Episode: 630 Total reward: 85.0 Explore P: 0.0526 Training Loss 3.4435\n",
      "Model Saved\n",
      "Episode: 632 Total reward: 52.0 Explore P: 0.0516 Training Loss 13.5831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 633 Total reward: 81.0 Explore P: 0.0515 Training Loss 4.3743\n",
      "Episode: 634 Total reward: 60.0 Explore P: 0.0513 Training Loss 3.4251\n",
      "Episode: 635 Total reward: 85.0 Explore P: 0.0513 Training Loss 2.4439\n",
      "Model Saved\n",
      "Episode: 636 Total reward: 82.0 Explore P: 0.0512 Training Loss 4.3621\n",
      "Episode: 637 Total reward: 88.0 Explore P: 0.0511 Training Loss 3.9158\n",
      "Episode: 638 Total reward: 93.0 Explore P: 0.0511 Training Loss 2.3438\n",
      "Episode: 639 Total reward: 50.0 Explore P: 0.0509 Training Loss 4.6538\n",
      "Episode: 640 Total reward: 58.0 Explore P: 0.0507 Training Loss 2.9801\n",
      "Model Saved\n",
      "Episode: 641 Total reward: 95.0 Explore P: 0.0507 Training Loss 1.5543\n",
      "Episode: 642 Total reward: 83.0 Explore P: 0.0506 Training Loss 8.5632\n",
      "Episode: 643 Total reward: 87.0 Explore P: 0.0506 Training Loss 1.4771\n",
      "Episode: 644 Total reward: 58.0 Explore P: 0.0504 Training Loss 6.4188\n",
      "Episode: 645 Total reward: 80.0 Explore P: 0.0504 Training Loss 5.1480\n",
      "Model Saved\n",
      "Episode: 646 Total reward: 65.0 Explore P: 0.0502 Training Loss 3.9244\n",
      "Episode: 647 Total reward: 39.0 Explore P: 0.0500 Training Loss 3.6483\n",
      "Episode: 648 Total reward: 94.0 Explore P: 0.0500 Training Loss 3.9466\n",
      "Episode: 649 Total reward: 88.0 Explore P: 0.0499 Training Loss 7.7539\n",
      "Episode: 650 Total reward: 94.0 Explore P: 0.0499 Training Loss 3.0848\n",
      "Model Saved\n",
      "Episode: 651 Total reward: 95.0 Explore P: 0.0499 Training Loss 1.8910\n",
      "Episode: 652 Total reward: 81.0 Explore P: 0.0498 Training Loss 3.1043\n",
      "Episode: 653 Total reward: 51.0 Explore P: 0.0496 Training Loss 8.0720\n",
      "Episode: 654 Total reward: 57.0 Explore P: 0.0495 Training Loss 8.6222\n",
      "Episode: 655 Total reward: 58.0 Explore P: 0.0493 Training Loss 3.3522\n",
      "Model Saved\n",
      "Episode: 656 Total reward: 95.0 Explore P: 0.0493 Training Loss 10.0371\n",
      "Episode: 657 Total reward: 93.0 Explore P: 0.0493 Training Loss 5.8774\n",
      "Episode: 658 Total reward: 85.0 Explore P: 0.0492 Training Loss 5.8925\n",
      "Episode: 659 Total reward: 80.0 Explore P: 0.0491 Training Loss 2.7525\n",
      "Episode: 660 Total reward: 83.0 Explore P: 0.0491 Training Loss 4.6743\n",
      "Model Saved\n",
      "Episode: 661 Total reward: 58.0 Explore P: 0.0489 Training Loss 6.4904\n",
      "Episode: 662 Total reward: 95.0 Explore P: 0.0489 Training Loss 4.7298\n",
      "Episode: 663 Total reward: 66.0 Explore P: 0.0488 Training Loss 3.2936\n",
      "Episode: 664 Total reward: 90.0 Explore P: 0.0487 Training Loss 3.5499\n",
      "Episode: 665 Total reward: 86.0 Explore P: 0.0487 Training Loss 3.3289\n",
      "Model Saved\n",
      "Episode: 666 Total reward: 33.0 Explore P: 0.0485 Training Loss 4.9271\n",
      "Episode: 667 Total reward: 95.0 Explore P: 0.0484 Training Loss 4.9630\n",
      "Episode: 668 Total reward: 86.0 Explore P: 0.0484 Training Loss 9.0568\n",
      "Episode: 669 Total reward: 82.0 Explore P: 0.0483 Training Loss 3.9045\n",
      "Episode: 670 Total reward: 67.0 Explore P: 0.0482 Training Loss 9.2546\n",
      "Model Saved\n",
      "Episode: 671 Total reward: 72.0 Explore P: 0.0481 Training Loss 5.3224\n",
      "Episode: 672 Total reward: 87.0 Explore P: 0.0480 Training Loss 9.2389\n",
      "Episode: 673 Total reward: 58.0 Explore P: 0.0479 Training Loss 13.7409\n",
      "Episode: 674 Total reward: 92.0 Explore P: 0.0478 Training Loss 2.7900\n",
      "Episode: 675 Total reward: 93.0 Explore P: 0.0478 Training Loss 2.9786\n",
      "Model Saved\n",
      "Episode: 676 Total reward: 77.0 Explore P: 0.0477 Training Loss 2.9991\n",
      "Episode: 677 Total reward: 94.0 Explore P: 0.0477 Training Loss 3.4716\n",
      "Episode: 679 Total reward: 95.0 Explore P: 0.0469 Training Loss 3.8150\n",
      "Episode: 680 Total reward: 95.0 Explore P: 0.0469 Training Loss 10.4873\n",
      "Model Saved\n",
      "Episode: 681 Total reward: 61.0 Explore P: 0.0468 Training Loss 3.6142\n",
      "Episode: 682 Total reward: 95.0 Explore P: 0.0467 Training Loss 7.2088\n",
      "Episode: 683 Total reward: 95.0 Explore P: 0.0467 Training Loss 1.5570\n",
      "Episode: 684 Total reward: 84.0 Explore P: 0.0467 Training Loss 3.7371\n",
      "Episode: 685 Total reward: 95.0 Explore P: 0.0466 Training Loss 3.6389\n",
      "Model Saved\n",
      "Episode: 686 Total reward: 61.0 Explore P: 0.0465 Training Loss 6.0825\n",
      "Episode: 687 Total reward: 89.0 Explore P: 0.0465 Training Loss 6.3770\n",
      "Episode: 688 Total reward: 95.0 Explore P: 0.0464 Training Loss 5.4849\n",
      "Episode: 689 Total reward: 84.0 Explore P: 0.0464 Training Loss 4.9095\n",
      "Episode: 690 Total reward: 85.0 Explore P: 0.0463 Training Loss 2.1004\n",
      "Model Saved\n",
      "Episode: 691 Total reward: 94.0 Explore P: 0.0463 Training Loss 3.7792\n",
      "Episode: 692 Total reward: 77.0 Explore P: 0.0462 Training Loss 2.3805\n",
      "Episode: 693 Total reward: 58.0 Explore P: 0.0461 Training Loss 1.5446\n",
      "Episode: 694 Total reward: 67.0 Explore P: 0.0460 Training Loss 5.8142\n",
      "Episode: 695 Total reward: 72.0 Explore P: 0.0459 Training Loss 3.8498\n",
      "Model Saved\n",
      "Episode: 696 Total reward: 61.0 Explore P: 0.0457 Training Loss 4.8999\n",
      "Episode: 697 Total reward: 64.0 Explore P: 0.0456 Training Loss 2.2178\n",
      "Episode: 698 Total reward: 76.0 Explore P: 0.0455 Training Loss 6.8338\n",
      "Episode: 699 Total reward: 75.0 Explore P: 0.0455 Training Loss 5.0536\n",
      "Episode: 700 Total reward: 95.0 Explore P: 0.0454 Training Loss 3.9372\n",
      "Model Saved\n",
      "Episode: 701 Total reward: 88.0 Explore P: 0.0454 Training Loss 4.8967\n",
      "Episode: 702 Total reward: 77.0 Explore P: 0.0453 Training Loss 10.6808\n",
      "Episode: 703 Total reward: 83.0 Explore P: 0.0452 Training Loss 5.3720\n",
      "Episode: 704 Total reward: 95.0 Explore P: 0.0452 Training Loss 3.8545\n",
      "Model Saved\n",
      "Episode: 706 Total reward: 54.0 Explore P: 0.0444 Training Loss 3.4617\n",
      "Episode: 707 Total reward: 62.0 Explore P: 0.0443 Training Loss 2.4066\n",
      "Episode: 708 Total reward: 72.0 Explore P: 0.0442 Training Loss 2.7841\n",
      "Episode: 709 Total reward: 43.0 Explore P: 0.0440 Training Loss 2.5686\n",
      "Episode: 710 Total reward: 58.0 Explore P: 0.0439 Training Loss 22.4767\n",
      "Model Saved\n",
      "Episode: 711 Total reward: 62.0 Explore P: 0.0438 Training Loss 3.1006\n",
      "Episode: 712 Total reward: 53.0 Explore P: 0.0436 Training Loss 2.3664\n",
      "Episode: 713 Total reward: 94.0 Explore P: 0.0436 Training Loss 2.5663\n",
      "Episode: 714 Total reward: 69.0 Explore P: 0.0435 Training Loss 3.7980\n",
      "Episode: 715 Total reward: 86.0 Explore P: 0.0435 Training Loss 7.2068\n",
      "Model Saved\n",
      "Episode: 716 Total reward: 60.0 Explore P: 0.0433 Training Loss 3.5076\n",
      "Episode: 717 Total reward: 95.0 Explore P: 0.0433 Training Loss 4.5601\n",
      "Episode: 718 Total reward: 95.0 Explore P: 0.0433 Training Loss 6.3139\n",
      "Episode: 719 Total reward: 40.0 Explore P: 0.0431 Training Loss 3.6418\n",
      "Episode: 720 Total reward: 58.0 Explore P: 0.0430 Training Loss 4.6259\n",
      "Model Saved\n",
      "Episode: 721 Total reward: 88.0 Explore P: 0.0430 Training Loss 8.3885\n",
      "Episode: 722 Total reward: 62.0 Explore P: 0.0429 Training Loss 9.8567\n",
      "Episode: 723 Total reward: 74.0 Explore P: 0.0428 Training Loss 4.3439\n",
      "Episode: 724 Total reward: 87.0 Explore P: 0.0427 Training Loss 2.7344\n",
      "Episode: 725 Total reward: 92.0 Explore P: 0.0427 Training Loss 3.7875\n",
      "Model Saved\n",
      "Episode: 726 Total reward: 83.0 Explore P: 0.0427 Training Loss 3.8258\n",
      "Episode: 727 Total reward: 52.0 Explore P: 0.0425 Training Loss 2.9296\n",
      "Episode: 728 Total reward: 61.0 Explore P: 0.0424 Training Loss 2.2333\n",
      "Episode: 730 Total reward: 92.0 Explore P: 0.0417 Training Loss 4.5585\n",
      "Model Saved\n",
      "Episode: 731 Total reward: 84.0 Explore P: 0.0417 Training Loss 1.6297\n",
      "Episode: 732 Total reward: 81.0 Explore P: 0.0416 Training Loss 3.1459\n",
      "Episode: 733 Total reward: 90.0 Explore P: 0.0416 Training Loss 2.8618\n",
      "Episode: 734 Total reward: 61.0 Explore P: 0.0415 Training Loss 7.5737\n",
      "Episode: 735 Total reward: 94.0 Explore P: 0.0414 Training Loss 4.6123\n",
      "Model Saved\n",
      "Episode: 736 Total reward: 95.0 Explore P: 0.0414 Training Loss 5.4099\n",
      "Episode: 737 Total reward: 84.0 Explore P: 0.0414 Training Loss 2.4401\n",
      "Episode: 738 Total reward: 87.0 Explore P: 0.0413 Training Loss 4.8449\n",
      "Episode: 739 Total reward: 76.0 Explore P: 0.0413 Training Loss 2.3394\n",
      "Episode: 740 Total reward: 38.0 Explore P: 0.0411 Training Loss 1.5951\n",
      "Model Saved\n",
      "Episode: 741 Total reward: 73.0 Explore P: 0.0410 Training Loss 2.7888\n",
      "Episode: 742 Total reward: 95.0 Explore P: 0.0410 Training Loss 4.2342\n",
      "Episode: 743 Total reward: 80.0 Explore P: 0.0410 Training Loss 2.6170\n",
      "Episode: 744 Total reward: 57.0 Explore P: 0.0409 Training Loss 6.7172\n",
      "Model Saved\n",
      "Episode: 746 Total reward: 94.0 Explore P: 0.0402 Training Loss 5.5249\n",
      "Episode: 747 Total reward: 57.0 Explore P: 0.0401 Training Loss 5.0790\n",
      "Episode: 748 Total reward: 87.0 Explore P: 0.0401 Training Loss 10.7468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 749 Total reward: 49.0 Explore P: 0.0399 Training Loss 6.9567\n",
      "Episode: 750 Total reward: 95.0 Explore P: 0.0399 Training Loss 5.5512\n",
      "Model Saved\n",
      "Episode: 751 Total reward: 40.0 Explore P: 0.0398 Training Loss 9.0889\n",
      "Episode: 752 Total reward: 90.0 Explore P: 0.0397 Training Loss 1.7553\n",
      "Episode: 753 Total reward: 85.0 Explore P: 0.0397 Training Loss 6.6505\n",
      "Episode: 754 Total reward: 80.0 Explore P: 0.0396 Training Loss 8.3840\n",
      "Episode: 755 Total reward: 85.0 Explore P: 0.0396 Training Loss 2.3633\n",
      "Model Saved\n",
      "Episode: 756 Total reward: 67.0 Explore P: 0.0395 Training Loss 2.2336\n",
      "Episode: 757 Total reward: 76.0 Explore P: 0.0394 Training Loss 3.1714\n",
      "Episode: 758 Total reward: 81.0 Explore P: 0.0394 Training Loss 4.7609\n",
      "Episode: 759 Total reward: 86.0 Explore P: 0.0393 Training Loss 4.6905\n",
      "Episode: 760 Total reward: 69.0 Explore P: 0.0392 Training Loss 4.2119\n",
      "Model Saved\n",
      "Episode: 761 Total reward: 90.0 Explore P: 0.0392 Training Loss 6.0302\n",
      "Episode: 762 Total reward: 95.0 Explore P: 0.0392 Training Loss 4.3458\n",
      "Episode: 763 Total reward: 58.0 Explore P: 0.0391 Training Loss 5.8103\n",
      "Episode: 764 Total reward: 75.0 Explore P: 0.0390 Training Loss 36.3529\n",
      "Episode: 765 Total reward: 57.0 Explore P: 0.0389 Training Loss 6.3391\n",
      "Model Saved\n",
      "Episode: 766 Total reward: 58.0 Explore P: 0.0388 Training Loss 4.9434\n",
      "Episode: 767 Total reward: 95.0 Explore P: 0.0388 Training Loss 6.2178\n",
      "Episode: 768 Total reward: 57.0 Explore P: 0.0387 Training Loss 3.6457\n",
      "Episode: 769 Total reward: 95.0 Explore P: 0.0387 Training Loss 8.1118\n",
      "Episode: 770 Total reward: 87.0 Explore P: 0.0386 Training Loss 46.8422\n",
      "Model Saved\n",
      "Episode: 771 Total reward: 77.0 Explore P: 0.0385 Training Loss 4.3344\n",
      "Episode: 772 Total reward: 90.0 Explore P: 0.0385 Training Loss 7.5937\n",
      "Episode: 773 Total reward: 90.0 Explore P: 0.0385 Training Loss 15.9053\n",
      "Episode: 774 Total reward: 52.0 Explore P: 0.0384 Training Loss 4.0957\n",
      "Episode: 775 Total reward: 67.0 Explore P: 0.0383 Training Loss 6.2920\n",
      "Model Saved\n",
      "Episode: 776 Total reward: 77.0 Explore P: 0.0382 Training Loss 7.4915\n",
      "Episode: 777 Total reward: 85.0 Explore P: 0.0382 Training Loss 2.6812\n",
      "Episode: 778 Total reward: 86.0 Explore P: 0.0381 Training Loss 5.6913\n",
      "Episode: 779 Total reward: 59.0 Explore P: 0.0380 Training Loss 3.3976\n",
      "Episode: 780 Total reward: 88.0 Explore P: 0.0380 Training Loss 64.6821\n",
      "Model Saved\n",
      "Episode: 781 Total reward: 61.0 Explore P: 0.0379 Training Loss 3.1021\n",
      "Episode: 782 Total reward: 83.0 Explore P: 0.0378 Training Loss 8.6647\n",
      "Episode: 783 Total reward: 58.0 Explore P: 0.0377 Training Loss 3.2405\n",
      "Episode: 784 Total reward: 91.0 Explore P: 0.0377 Training Loss 6.1781\n",
      "Episode: 785 Total reward: 65.0 Explore P: 0.0376 Training Loss 5.6599\n",
      "Model Saved\n",
      "Episode: 786 Total reward: 87.0 Explore P: 0.0376 Training Loss 5.8689\n",
      "Episode: 787 Total reward: 66.0 Explore P: 0.0375 Training Loss 3.9132\n",
      "Episode: 788 Total reward: 18.0 Explore P: 0.0373 Training Loss 6.1729\n",
      "Episode: 789 Total reward: 77.0 Explore P: 0.0372 Training Loss 5.8369\n",
      "Episode: 790 Total reward: 82.0 Explore P: 0.0372 Training Loss 3.0362\n",
      "Model Saved\n",
      "Episode: 791 Total reward: 89.0 Explore P: 0.0372 Training Loss 3.0236\n",
      "Episode: 792 Total reward: 77.0 Explore P: 0.0371 Training Loss 3.1047\n",
      "Episode: 793 Total reward: 61.0 Explore P: 0.0370 Training Loss 17.7433\n",
      "Episode: 794 Total reward: 83.0 Explore P: 0.0370 Training Loss 8.1972\n",
      "Episode: 795 Total reward: 95.0 Explore P: 0.0369 Training Loss 3.0482\n",
      "Model Saved\n",
      "Episode: 797 Total reward: 95.0 Explore P: 0.0364 Training Loss 4.0482\n",
      "Episode: 798 Total reward: 95.0 Explore P: 0.0364 Training Loss 4.0801\n",
      "Episode: 799 Total reward: 57.0 Explore P: 0.0363 Training Loss 2.2516\n",
      "Episode: 800 Total reward: 82.0 Explore P: 0.0362 Training Loss 2.4911\n",
      "Model Saved\n",
      "Episode: 801 Total reward: 85.0 Explore P: 0.0362 Training Loss 3.8081\n",
      "Episode: 802 Total reward: 95.0 Explore P: 0.0362 Training Loss 2.1612\n",
      "Episode: 803 Total reward: 79.0 Explore P: 0.0361 Training Loss 5.1630\n",
      "Episode: 804 Total reward: 46.0 Explore P: 0.0360 Training Loss 5.0285\n",
      "Episode: 805 Total reward: 94.0 Explore P: 0.0360 Training Loss 3.4545\n",
      "Model Saved\n",
      "Episode: 806 Total reward: 20.0 Explore P: 0.0358 Training Loss 4.6059\n",
      "Episode: 807 Total reward: 46.0 Explore P: 0.0357 Training Loss 2.0832\n",
      "Episode: 808 Total reward: 76.0 Explore P: 0.0356 Training Loss 7.2577\n",
      "Episode: 809 Total reward: 93.0 Explore P: 0.0356 Training Loss 2.6106\n",
      "Episode: 810 Total reward: 41.0 Explore P: 0.0355 Training Loss 5.2585\n",
      "Model Saved\n",
      "Episode: 811 Total reward: 95.0 Explore P: 0.0355 Training Loss 3.0836\n",
      "Episode: 812 Total reward: 58.0 Explore P: 0.0354 Training Loss 2.4316\n",
      "Episode: 813 Total reward: 58.0 Explore P: 0.0353 Training Loss 7.1418\n",
      "Episode: 814 Total reward: 68.0 Explore P: 0.0352 Training Loss 3.3971\n",
      "Episode: 815 Total reward: 73.0 Explore P: 0.0351 Training Loss 4.5391\n",
      "Model Saved\n",
      "Episode: 816 Total reward: 95.0 Explore P: 0.0351 Training Loss 3.8989\n",
      "Episode: 817 Total reward: 82.0 Explore P: 0.0351 Training Loss 2.9926\n",
      "Episode: 818 Total reward: 81.0 Explore P: 0.0350 Training Loss 8.5828\n",
      "Episode: 819 Total reward: 57.0 Explore P: 0.0349 Training Loss 2.3488\n",
      "Episode: 820 Total reward: 58.0 Explore P: 0.0348 Training Loss 8.7587\n",
      "Model Saved\n",
      "Episode: 821 Total reward: 71.0 Explore P: 0.0348 Training Loss 4.4884\n",
      "Episode: 822 Total reward: 92.0 Explore P: 0.0348 Training Loss 2.1098\n",
      "Episode: 823 Total reward: 95.0 Explore P: 0.0347 Training Loss 3.1350\n",
      "Episode: 824 Total reward: 56.0 Explore P: 0.0346 Training Loss 3.7458\n",
      "Episode: 825 Total reward: 58.0 Explore P: 0.0345 Training Loss 2.9252\n",
      "Model Saved\n",
      "Episode: 826 Total reward: 58.0 Explore P: 0.0345 Training Loss 3.4751\n",
      "Episode: 827 Total reward: 77.0 Explore P: 0.0344 Training Loss 88.9916\n",
      "Episode: 828 Total reward: 89.0 Explore P: 0.0344 Training Loss 3.2782\n",
      "Episode: 829 Total reward: 82.0 Explore P: 0.0343 Training Loss 134.1861\n",
      "Episode: 830 Total reward: 74.0 Explore P: 0.0343 Training Loss 3.8572\n",
      "Model Saved\n",
      "Episode: 831 Total reward: 90.0 Explore P: 0.0342 Training Loss 5.4570\n",
      "Episode: 832 Total reward: 81.0 Explore P: 0.0342 Training Loss 3.0523\n",
      "Episode: 833 Total reward: 67.0 Explore P: 0.0341 Training Loss 4.5965\n",
      "Episode: 834 Total reward: 69.0 Explore P: 0.0341 Training Loss 4.7961\n",
      "Episode: 835 Total reward: 79.0 Explore P: 0.0340 Training Loss 1.7260\n",
      "Model Saved\n",
      "Episode: 836 Total reward: 77.0 Explore P: 0.0339 Training Loss 4.2266\n",
      "Episode: 837 Total reward: 95.0 Explore P: 0.0339 Training Loss 2.5499\n",
      "Episode: 838 Total reward: 76.0 Explore P: 0.0339 Training Loss 5.1919\n",
      "Episode: 839 Total reward: 77.0 Explore P: 0.0338 Training Loss 10.1890\n",
      "Episode: 840 Total reward: 82.0 Explore P: 0.0338 Training Loss 3.3036\n",
      "Model Saved\n",
      "Episode: 841 Total reward: 60.0 Explore P: 0.0337 Training Loss 7.7954\n",
      "Episode: 842 Total reward: 74.0 Explore P: 0.0336 Training Loss 5.7441\n",
      "Episode: 843 Total reward: 60.0 Explore P: 0.0335 Training Loss 6.4858\n",
      "Episode: 844 Total reward: 95.0 Explore P: 0.0335 Training Loss 3.7994\n",
      "Episode: 845 Total reward: 89.0 Explore P: 0.0335 Training Loss 3.9698\n",
      "Model Saved\n",
      "Episode: 846 Total reward: 64.0 Explore P: 0.0334 Training Loss 7.1942\n",
      "Episode: 847 Total reward: 83.0 Explore P: 0.0334 Training Loss 5.0722\n",
      "Episode: 849 Total reward: 57.0 Explore P: 0.0328 Training Loss 6.1432\n",
      "Episode: 850 Total reward: 95.0 Explore P: 0.0328 Training Loss 5.3081\n",
      "Model Saved\n",
      "Episode: 852 Total reward: 93.0 Explore P: 0.0324 Training Loss 9.1186\n",
      "Episode: 853 Total reward: 91.0 Explore P: 0.0323 Training Loss 19.5021\n",
      "Episode: 854 Total reward: 62.0 Explore P: 0.0323 Training Loss 3.0597\n",
      "Episode: 855 Total reward: 77.0 Explore P: 0.0322 Training Loss 2.1372\n",
      "Model Saved\n",
      "Episode: 856 Total reward: 72.0 Explore P: 0.0322 Training Loss 6.9478\n",
      "Episode: 857 Total reward: 47.0 Explore P: 0.0321 Training Loss 7.9764\n",
      "Episode: 858 Total reward: 57.0 Explore P: 0.0320 Training Loss 39.9221\n",
      "Episode: 859 Total reward: 94.0 Explore P: 0.0320 Training Loss 9.8837\n",
      "Episode: 860 Total reward: 95.0 Explore P: 0.0320 Training Loss 6.2293\n",
      "Model Saved\n",
      "Episode: 862 Total reward: 84.0 Explore P: 0.0315 Training Loss 5.1499\n",
      "Episode: 863 Total reward: 80.0 Explore P: 0.0314 Training Loss 4.2731\n",
      "Episode: 864 Total reward: 82.0 Explore P: 0.0314 Training Loss 5.0489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 865 Total reward: 64.0 Explore P: 0.0313 Training Loss 6.7560\n",
      "Model Saved\n",
      "Episode: 866 Total reward: 81.0 Explore P: 0.0313 Training Loss 3.3870\n",
      "Episode: 867 Total reward: 91.0 Explore P: 0.0313 Training Loss 4.5489\n",
      "Episode: 868 Total reward: 74.0 Explore P: 0.0312 Training Loss 2.8897\n",
      "Episode: 869 Total reward: 60.0 Explore P: 0.0311 Training Loss 4.1775\n",
      "Episode: 870 Total reward: 77.0 Explore P: 0.0311 Training Loss 9.6188\n",
      "Model Saved\n",
      "Episode: 871 Total reward: 88.0 Explore P: 0.0311 Training Loss 2.9421\n",
      "Episode: 872 Total reward: 60.0 Explore P: 0.0310 Training Loss 11.4547\n",
      "Episode: 873 Total reward: 92.0 Explore P: 0.0310 Training Loss 26.0982\n",
      "Episode: 875 Total reward: -15.0 Explore P: 0.0304 Training Loss 7.6093\n",
      "Model Saved\n",
      "Episode: 876 Total reward: 72.0 Explore P: 0.0303 Training Loss 3.0582\n",
      "Episode: 877 Total reward: 84.0 Explore P: 0.0303 Training Loss 7.7211\n",
      "Episode: 878 Total reward: 82.0 Explore P: 0.0302 Training Loss 1.6527\n",
      "Episode: 879 Total reward: 83.0 Explore P: 0.0302 Training Loss 2.7173\n",
      "Episode: 880 Total reward: 95.0 Explore P: 0.0302 Training Loss 6.1693\n",
      "Model Saved\n",
      "Episode: 881 Total reward: 81.0 Explore P: 0.0301 Training Loss 4.5029\n",
      "Episode: 882 Total reward: 77.0 Explore P: 0.0301 Training Loss 2.4239\n",
      "Episode: 883 Total reward: 90.0 Explore P: 0.0301 Training Loss 2.8915\n",
      "Episode: 884 Total reward: 7.0 Explore P: 0.0299 Training Loss 5.7931\n",
      "Model Saved\n",
      "Episode: 887 Total reward: 88.0 Explore P: 0.0291 Training Loss 5.0581\n",
      "Episode: 888 Total reward: 84.0 Explore P: 0.0291 Training Loss 2.1384\n",
      "Episode: 889 Total reward: 93.0 Explore P: 0.0291 Training Loss 2.2330\n",
      "Episode: 890 Total reward: 95.0 Explore P: 0.0291 Training Loss 6.1260\n",
      "Model Saved\n",
      "Episode: 891 Total reward: 45.0 Explore P: 0.0290 Training Loss 6.4943\n",
      "Episode: 892 Total reward: 77.0 Explore P: 0.0289 Training Loss 8.8924\n",
      "Episode: 893 Total reward: 95.0 Explore P: 0.0289 Training Loss 3.4389\n",
      "Episode: 894 Total reward: 57.0 Explore P: 0.0288 Training Loss 4.3871\n",
      "Episode: 895 Total reward: 42.0 Explore P: 0.0287 Training Loss 3.2790\n",
      "Model Saved\n",
      "Episode: 896 Total reward: 83.0 Explore P: 0.0287 Training Loss 5.6479\n",
      "Episode: 897 Total reward: 83.0 Explore P: 0.0287 Training Loss 3.2087\n",
      "Episode: 898 Total reward: 91.0 Explore P: 0.0287 Training Loss 2.7591\n",
      "Episode: 899 Total reward: 68.0 Explore P: 0.0286 Training Loss 2.3178\n",
      "Model Saved\n",
      "Episode: 901 Total reward: 95.0 Explore P: 0.0282 Training Loss 2.1924\n",
      "Episode: 902 Total reward: 93.0 Explore P: 0.0282 Training Loss 4.8263\n",
      "Episode: 903 Total reward: 95.0 Explore P: 0.0282 Training Loss 2.1733\n",
      "Episode: 904 Total reward: 73.0 Explore P: 0.0281 Training Loss 5.7469\n",
      "Episode: 905 Total reward: 36.0 Explore P: 0.0280 Training Loss 3.0782\n",
      "Model Saved\n",
      "Episode: 906 Total reward: 59.0 Explore P: 0.0280 Training Loss 44.1837\n",
      "Episode: 907 Total reward: 84.0 Explore P: 0.0279 Training Loss 3.9524\n",
      "Episode: 908 Total reward: 58.0 Explore P: 0.0279 Training Loss 2.8692\n",
      "Episode: 909 Total reward: 77.0 Explore P: 0.0278 Training Loss 13.1991\n",
      "Episode: 910 Total reward: 88.0 Explore P: 0.0278 Training Loss 3.1711\n",
      "Model Saved\n",
      "Episode: 911 Total reward: 77.0 Explore P: 0.0278 Training Loss 2.1738\n",
      "Episode: 912 Total reward: 68.0 Explore P: 0.0277 Training Loss 2.9564\n",
      "Episode: 913 Total reward: 95.0 Explore P: 0.0277 Training Loss 3.6793\n",
      "Episode: 914 Total reward: 53.0 Explore P: 0.0276 Training Loss 3.6897\n",
      "Episode: 915 Total reward: 77.0 Explore P: 0.0276 Training Loss 2.3145\n",
      "Model Saved\n",
      "Episode: 916 Total reward: 94.0 Explore P: 0.0276 Training Loss 4.3089\n",
      "Episode: 917 Total reward: 58.0 Explore P: 0.0275 Training Loss 3.5891\n",
      "Episode: 918 Total reward: 58.0 Explore P: 0.0274 Training Loss 3.0312\n",
      "Episode: 919 Total reward: 83.0 Explore P: 0.0274 Training Loss 1.5642\n",
      "Episode: 920 Total reward: 57.0 Explore P: 0.0273 Training Loss 3.9815\n",
      "Model Saved\n",
      "Episode: 921 Total reward: 95.0 Explore P: 0.0273 Training Loss 5.6585\n",
      "Episode: 922 Total reward: 95.0 Explore P: 0.0273 Training Loss 4.7238\n",
      "Episode: 923 Total reward: 58.0 Explore P: 0.0273 Training Loss 2.4553\n",
      "Episode: 925 Total reward: 88.0 Explore P: 0.0269 Training Loss 3.3423\n",
      "Model Saved\n",
      "Episode: 926 Total reward: 58.0 Explore P: 0.0268 Training Loss 9.3647\n",
      "Episode: 927 Total reward: 86.0 Explore P: 0.0268 Training Loss 2.6329\n",
      "Episode: 928 Total reward: 73.0 Explore P: 0.0268 Training Loss 2.5877\n",
      "Episode: 929 Total reward: 58.0 Explore P: 0.0267 Training Loss 7.3223\n",
      "Episode: 930 Total reward: 77.0 Explore P: 0.0267 Training Loss 4.5238\n",
      "Model Saved\n",
      "Episode: 931 Total reward: 58.0 Explore P: 0.0266 Training Loss 3.5870\n",
      "Episode: 932 Total reward: 73.0 Explore P: 0.0266 Training Loss 2.5040\n",
      "Episode: 933 Total reward: 85.0 Explore P: 0.0265 Training Loss 2.5980\n",
      "Episode: 934 Total reward: 58.0 Explore P: 0.0265 Training Loss 2.5799\n",
      "Episode: 935 Total reward: 73.0 Explore P: 0.0264 Training Loss 2.2122\n",
      "Model Saved\n",
      "Episode: 936 Total reward: 77.0 Explore P: 0.0264 Training Loss 4.0876\n",
      "Episode: 937 Total reward: 77.0 Explore P: 0.0263 Training Loss 1.2504\n",
      "Episode: 938 Total reward: 53.0 Explore P: 0.0263 Training Loss 5.5710\n",
      "Episode: 939 Total reward: 77.0 Explore P: 0.0262 Training Loss 4.7773\n",
      "Episode: 940 Total reward: 77.0 Explore P: 0.0262 Training Loss 3.4299\n",
      "Model Saved\n",
      "Episode: 941 Total reward: 95.0 Explore P: 0.0262 Training Loss 3.9051\n",
      "Episode: 942 Total reward: 61.0 Explore P: 0.0261 Training Loss 16.6438\n",
      "Episode: 943 Total reward: 81.0 Explore P: 0.0261 Training Loss 2.2825\n",
      "Episode: 945 Total reward: 65.0 Explore P: 0.0257 Training Loss 5.4930\n",
      "Model Saved\n",
      "Episode: 946 Total reward: 95.0 Explore P: 0.0257 Training Loss 3.2863\n",
      "Episode: 947 Total reward: 95.0 Explore P: 0.0257 Training Loss 2.6254\n",
      "Episode: 948 Total reward: 64.0 Explore P: 0.0257 Training Loss 10.8446\n",
      "Episode: 949 Total reward: 88.0 Explore P: 0.0256 Training Loss 3.8505\n",
      "Episode: 950 Total reward: 86.0 Explore P: 0.0256 Training Loss 2.7442\n",
      "Model Saved\n",
      "Episode: 951 Total reward: 79.0 Explore P: 0.0256 Training Loss 10.1855\n",
      "Episode: 952 Total reward: 81.0 Explore P: 0.0255 Training Loss 7.4685\n",
      "Episode: 953 Total reward: 73.0 Explore P: 0.0255 Training Loss 2.5199\n",
      "Episode: 954 Total reward: 95.0 Explore P: 0.0255 Training Loss 3.8732\n",
      "Episode: 955 Total reward: 95.0 Explore P: 0.0255 Training Loss 5.6917\n",
      "Model Saved\n",
      "Episode: 956 Total reward: 39.0 Explore P: 0.0254 Training Loss 4.5604\n",
      "Episode: 957 Total reward: 68.0 Explore P: 0.0254 Training Loss 6.5947\n",
      "Episode: 959 Total reward: 85.0 Explore P: 0.0250 Training Loss 2.0573\n",
      "Model Saved\n",
      "Episode: 961 Total reward: 69.0 Explore P: 0.0247 Training Loss 5.3992\n",
      "Episode: 962 Total reward: 89.0 Explore P: 0.0247 Training Loss 3.1217\n",
      "Episode: 963 Total reward: 67.0 Explore P: 0.0246 Training Loss 6.7880\n",
      "Episode: 964 Total reward: 77.0 Explore P: 0.0246 Training Loss 2.9551\n",
      "Episode: 965 Total reward: 54.0 Explore P: 0.0245 Training Loss 4.0714\n",
      "Model Saved\n",
      "Episode: 966 Total reward: 73.0 Explore P: 0.0245 Training Loss 69.6685\n",
      "Episode: 967 Total reward: 95.0 Explore P: 0.0245 Training Loss 5.5870\n",
      "Episode: 968 Total reward: 81.0 Explore P: 0.0245 Training Loss 8.6191\n",
      "Episode: 969 Total reward: 83.0 Explore P: 0.0244 Training Loss 4.1787\n",
      "Episode: 970 Total reward: 93.0 Explore P: 0.0244 Training Loss 12.8812\n",
      "Model Saved\n",
      "Episode: 971 Total reward: 76.0 Explore P: 0.0244 Training Loss 4.8148\n",
      "Episode: 972 Total reward: 64.0 Explore P: 0.0243 Training Loss 6.4489\n",
      "Episode: 973 Total reward: 80.0 Explore P: 0.0243 Training Loss 6.0036\n",
      "Episode: 974 Total reward: 70.0 Explore P: 0.0243 Training Loss 2.0095\n",
      "Episode: 975 Total reward: 95.0 Explore P: 0.0243 Training Loss 4.6905\n",
      "Model Saved\n",
      "Episode: 976 Total reward: 73.0 Explore P: 0.0242 Training Loss 4.1365\n",
      "Episode: 977 Total reward: 81.0 Explore P: 0.0242 Training Loss 5.3838\n",
      "Episode: 978 Total reward: 77.0 Explore P: 0.0242 Training Loss 5.8311\n",
      "Episode: 979 Total reward: 80.0 Explore P: 0.0241 Training Loss 2.6603\n",
      "Episode: 980 Total reward: 85.0 Explore P: 0.0241 Training Loss 6.6119\n",
      "Model Saved\n",
      "Episode: 981 Total reward: 53.0 Explore P: 0.0240 Training Loss 2.6901\n",
      "Episode: 982 Total reward: 74.0 Explore P: 0.0240 Training Loss 3.7447\n",
      "Episode: 983 Total reward: 84.0 Explore P: 0.0240 Training Loss 3.1561\n",
      "Episode: 984 Total reward: 77.0 Explore P: 0.0240 Training Loss 2.1613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 985 Total reward: 54.0 Explore P: 0.0239 Training Loss 5.1025\n",
      "Model Saved\n",
      "Episode: 986 Total reward: 92.0 Explore P: 0.0239 Training Loss 1.9653\n",
      "Episode: 987 Total reward: 77.0 Explore P: 0.0238 Training Loss 1.7970\n",
      "Episode: 988 Total reward: 86.0 Explore P: 0.0238 Training Loss 3.4187\n",
      "Episode: 989 Total reward: 85.0 Explore P: 0.0238 Training Loss 4.9621\n",
      "Episode: 990 Total reward: 95.0 Explore P: 0.0238 Training Loss 3.5199\n",
      "Model Saved\n",
      "Episode: 991 Total reward: 86.0 Explore P: 0.0238 Training Loss 5.0193\n",
      "Episode: 992 Total reward: 61.0 Explore P: 0.0237 Training Loss 6.4897\n",
      "Episode: 993 Total reward: 73.0 Explore P: 0.0237 Training Loss 10.8818\n",
      "Episode: 994 Total reward: 38.0 Explore P: 0.0236 Training Loss 6.0405\n",
      "Episode: 995 Total reward: 95.0 Explore P: 0.0236 Training Loss 2.2631\n",
      "Model Saved\n",
      "Episode: 996 Total reward: 95.0 Explore P: 0.0236 Training Loss 2.1976\n",
      "Episode: 997 Total reward: 94.0 Explore P: 0.0236 Training Loss 4.1743\n",
      "Episode: 998 Total reward: 95.0 Explore P: 0.0236 Training Loss 3.9248\n",
      "Episode: 999 Total reward: 73.0 Explore P: 0.0235 Training Loss 2.7803\n"
     ]
    }
   ],
   "source": [
    "#The saver will save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        #Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #Initialize the decay rate that will be used to reduce epislon over time\n",
    "        decay_step = 0\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            #Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Init the game\n",
    "            game.init()\n",
    "            \n",
    "            #Initialize list containing rewards per episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            #Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            \"\"\"\n",
    "            The stacked frame function will also call and make use of the preprocess function.\n",
    "            The function will crop and reszie the image and then stack them in a double ended queue\n",
    "            as a way to overcome the temporal limitation problem.\n",
    "            \"\"\"\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                #increase decay_step\n",
    "                decay_step += 1\n",
    "                \n",
    "                #Predict an action to take and then implement that action\n",
    "                action, explore_probability = predict_action(explore_start, \n",
    "                                                explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                \n",
    "                #Perform the action and get the next_state, reward, and done information\n",
    "                reward = game.make_action(action)\n",
    "                \n",
    "                #Check if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward , store the reward for each action in a list\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                #If the game is finished\n",
    "                if done:\n",
    "                    #The episode end so no next state\n",
    "                    next_state = np.zeros((84,84), dtype = np.int)\n",
    "                    \n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    #Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "                    \n",
    "                    #Get the total reward of the episode by summing up the rewards for each action\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print('Episode: {}'.format(episode),\n",
    "                                  'Total reward: {}'.format(total_reward),\n",
    "                                  'Explore P: {:.4f}'.format(explore_probability),\n",
    "                                'Training Loss {:.4f}'.format(loss))\n",
    "                    \n",
    "                   # rewards_list.append((episode, total_reward))\n",
    "                    \n",
    "                    # Store transition <st,at,rt+1,st+1> in memory D\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                else: \n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    #Stack the frames of the next state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    #Add Experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    #st+1 is now our current state\n",
    "                    state = next_state\n",
    "                    \n",
    "                    ##########PART OF THE ALGORITHUM THAT LEARNS######################\n",
    "                    #Obtain random mini-batch from memory\n",
    "                    #Samples memory\n",
    "                    batch = memory.sample(batch_size)\n",
    "                    states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                    actions_mb = np.array([each[1] for each in batch])\n",
    "                    rewards_mb = np.array([each[2] for each in batch])\n",
    "                    next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                    dones_mb = np.array([each[4] for each in batch])\n",
    "                    \n",
    "                    target_Qs_batch = []\n",
    "                    \n",
    "                    #Get Q Values for the next state\n",
    "                    Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                    \n",
    "                    #Set Q_traget = r if the episode ends at s+1, else set Q_target = r + gamma*maxQ(s', a')\n",
    "                    for i in range(0, len(batch)):\n",
    "                        terminal = dones_mb[i]\n",
    "                        \n",
    "                        #If we are in a terminal state, only equals reward\n",
    "                        if terminal:\n",
    "                            target_Qs_batch.append(rewards_mb[i])\n",
    "                        else:\n",
    "                            target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                            target_Qs_batch.append(target)\n",
    "                    \n",
    "                    targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                    \n",
    "                    loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb})\n",
    "                    \n",
    "                    # Write TF Summaries\n",
    "                    summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                       DQNetwork.target_Q: targets_mb,\n",
    "                                                       DQNetwork.actions_: actions_mb})\n",
    "                    writer.add_summary(summary, episode)\n",
    "                    writer.flush()\n",
    "                    \n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                    save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                    print(\"Model Saved\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  74.0\n",
      "Score:  53.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  79.0\n",
      "Score:  75.0\n",
      "Score:  88.0\n",
      "Score:  57.0\n",
      "Score:  83.0\n",
      "Score:  68.0\n",
      "Score:  73.0\n",
      "Score:  84.0\n",
      "Score:  92.0\n",
      "Score:  93.0\n",
      "Score:  36.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  1.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  75.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  74.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  58.0\n",
      "Score:  54.0\n",
      "Score:  65.0\n",
      "Score:  90.0\n",
      "Score:  92.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  72.0\n",
      "Score:  84.0\n",
      "Score:  77.0\n",
      "Score:  81.0\n",
      "Score:  74.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  88.0\n",
      "Score:  80.0\n",
      "Score:  84.0\n",
      "Score:  74.0\n",
      "Score:  84.0\n",
      "Score:  53.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  69.0\n",
      "Score:  88.0\n",
      "Score:  67.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  85.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  65.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  87.0\n",
      "Score:  93.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  16.0\n",
      "Score:  76.0\n",
      "Score:  85.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  93.0\n",
      "Score:  54.0\n",
      "Score:  86.0\n",
      "Score:  91.0\n",
      "Score:  67.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  -350.0\n",
      "Score:  72.0\n",
      "Score:  87.0\n",
      "Score:  81.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  88.0\n",
      "Score:  82.0\n",
      "Score:  75.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  74.0\n",
      "Score:  54.0\n",
      "Score:  85.0\n",
      "Score:  79.0\n",
      "Score:  84.0\n",
      "Score:  70.0\n",
      "Score:  92.0\n",
      "Score:  58.0\n",
      "Score:  95.0\n",
      "Score:  58.0\n",
      "Score:  67.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  93.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  35.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  93.0\n",
      "Score:  93.0\n",
      "Score:  74.0\n",
      "Score:  94.0\n",
      "Score:  16.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  89.0\n",
      "Score:  95.0\n",
      "Score:  57.0\n",
      "Score:  82.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  58.0\n",
      "Score:  92.0\n",
      "Score:  77.0\n",
      "Score:  86.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  53.0\n",
      "Score:  73.0\n",
      "Score:  70.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  91.0\n",
      "Score:  93.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  69.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  67.0\n",
      "Score:  67.0\n",
      "Score:  75.0\n",
      "Score:  77.0\n",
      "Score:  74.0\n",
      "Score:  93.0\n",
      "Score:  68.0\n",
      "Score:  87.0\n",
      "Score:  73.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  74.0\n",
      "Score:  85.0\n",
      "Score:  89.0\n",
      "Score:  73.0\n",
      "Score:  92.0\n",
      "Score:  77.0\n",
      "Score:  50.0\n",
      "Score:  75.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  86.0\n",
      "Score:  95.0\n",
      "Score:  79.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  87.0\n",
      "Score:  57.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  92.0\n",
      "Score:  67.0\n",
      "Score:  81.0\n",
      "Score:  93.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  57.0\n",
      "Score:  85.0\n",
      "Score:  86.0\n",
      "Score:  82.0\n",
      "Score:  75.0\n",
      "Score:  83.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  77.0\n",
      "Score:  86.0\n",
      "Score:  89.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  88.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  73.0\n",
      "Score:  79.0\n",
      "Score:  67.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  67.0\n",
      "Score:  54.0\n",
      "Score:  94.0\n",
      "Score:  79.0\n",
      "Score:  57.0\n",
      "Score:  81.0\n",
      "Score:  -3.0\n",
      "Score:  81.0\n",
      "Score:  84.0\n",
      "Score:  81.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  35.0\n",
      "Score:  77.0\n",
      "Score:  67.0\n",
      "Score:  33.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  74.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  92.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  80.0\n",
      "Score:  77.0\n",
      "Score:  89.0\n",
      "Score:  80.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  82.0\n",
      "Score:  67.0\n",
      "Score:  85.0\n",
      "Score:  54.0\n",
      "Score:  88.0\n",
      "Score:  85.0\n",
      "Score:  58.0\n",
      "Score:  92.0\n",
      "Score:  85.0\n",
      "Score:  57.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  84.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  74.0\n",
      "Score:  88.0\n",
      "Score:  53.0\n",
      "Score:  80.0\n",
      "Score:  83.0\n",
      "Score:  35.0\n",
      "Score:  79.0\n",
      "Score:  94.0\n",
      "Score:  67.0\n",
      "Score:  73.0\n",
      "Score:  65.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  86.0\n",
      "Score:  91.0\n",
      "Score:  77.0\n",
      "Score:  91.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  69.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  92.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  35.0\n",
      "Score:  58.0\n",
      "Score:  85.0\n",
      "Score:  88.0\n",
      "Score:  84.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  94.0\n",
      "Score:  85.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  80.0\n",
      "Score:  87.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  79.0\n",
      "Score:  81.0\n",
      "Score:  75.0\n",
      "Score:  84.0\n",
      "Score:  68.0\n",
      "Score:  81.0\n",
      "Score:  82.0\n",
      "Score:  67.0\n",
      "Score:  85.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  -3.0\n",
      "Score:  82.0\n",
      "Score:  91.0\n",
      "Score:  68.0\n",
      "Score:  88.0\n",
      "Score:  68.0\n",
      "Score:  54.0\n",
      "Score:  90.0\n",
      "Score:  77.0\n",
      "Score:  80.0\n",
      "Score:  58.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  94.0\n",
      "Score:  73.0\n",
      "Score:  81.0\n",
      "Score:  57.0\n",
      "Score:  73.0\n",
      "Score:  57.0\n",
      "Score:  85.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  71.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  53.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  91.0\n",
      "Score:  68.0\n",
      "Score:  95.0\n",
      "Score:  70.0\n",
      "Score:  83.0\n",
      "Score:  69.0\n",
      "Score:  67.0\n",
      "Score:  58.0\n",
      "Score:  85.0\n",
      "Score:  86.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  57.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  70.0\n",
      "Score:  82.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  70.0\n",
      "Score:  72.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  75.0\n",
      "Score:  58.0\n",
      "Score:  75.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  91.0\n",
      "Score:  93.0\n",
      "Score:  77.0\n",
      "Score:  79.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  54.0\n",
      "Score:  92.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  82.0\n",
      "Score:  85.0\n",
      "Score:  67.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  88.0\n",
      "Score:  91.0\n",
      "Score:  95.0\n",
      "Score:  57.0\n",
      "Score:  83.0\n",
      "Score:  87.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  91.0\n",
      "Score:  83.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  68.0\n",
      "Score:  86.0\n",
      "Score:  77.0\n",
      "Score:  53.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  82.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  53.0\n",
      "Score:  75.0\n",
      "Score:  83.0\n",
      "Score:  87.0\n",
      "Score:  89.0\n",
      "Score:  35.0\n",
      "Score:  57.0\n",
      "Score:  87.0\n",
      "Score:  79.0\n",
      "Score:  77.0\n",
      "Score:  68.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  54.0\n",
      "Score:  83.0\n",
      "Score:  83.0\n",
      "Score:  75.0\n",
      "Score:  86.0\n",
      "Score:  54.0\n",
      "Score:  80.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  92.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  75.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  67.0\n",
      "Score:  86.0\n",
      "Score:  86.0\n",
      "Score:  91.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  94.0\n",
      "Score:  30.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  58.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  84.0\n",
      "Score:  86.0\n",
      "Score:  73.0\n",
      "Score:  84.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  74.0\n",
      "Score:  73.0\n",
      "Score:  89.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  93.0\n",
      "Score:  54.0\n",
      "Score:  75.0\n",
      "Score:  77.0\n",
      "Score:  88.0\n",
      "Score:  86.0\n",
      "Score:  77.0\n",
      "Score:  80.0\n",
      "Score:  77.0\n",
      "Score:  94.0\n",
      "Score:  86.0\n",
      "Score:  83.0\n",
      "Score:  68.0\n",
      "Score:  70.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  73.0\n",
      "Score:  68.0\n",
      "Score:  68.0\n",
      "Score:  -370.0\n",
      "Score:  94.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  54.0\n",
      "Score:  80.0\n",
      "Score:  68.0\n",
      "Score:  77.0\n",
      "Score:  69.0\n",
      "Score:  83.0\n",
      "Score:  86.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  80.0\n",
      "Score:  77.0\n",
      "Score:  86.0\n",
      "Score:  70.0\n",
      "Score:  74.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  -360.0\n",
      "Score:  93.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  57.0\n",
      "Score:  81.0\n",
      "Score:  54.0\n",
      "Score:  94.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  93.0\n",
      "Score:  84.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  53.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  85.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  50.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  80.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  67.0\n",
      "Score:  77.0\n",
      "Score:  57.0\n",
      "Score:  84.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  69.0\n",
      "Score:  82.0\n",
      "Score:  85.0\n",
      "Score:  82.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  86.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  57.0\n",
      "Score:  67.0\n",
      "Score:  86.0\n",
      "Score:  81.0\n",
      "Score:  80.0\n",
      "Score:  73.0\n",
      "Score:  85.0\n",
      "Score:  50.0\n",
      "Score:  54.0\n",
      "Score:  80.0\n",
      "Score:  54.0\n",
      "Score:  85.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  35.0\n",
      "Score:  86.0\n",
      "Score:  57.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  54.0\n",
      "Score:  89.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  68.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  89.0\n",
      "Score:  77.0\n",
      "Score:  72.0\n",
      "Score:  83.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  81.0\n",
      "Score:  65.0\n",
      "Score:  73.0\n",
      "Score:  57.0\n",
      "Score:  19.0\n",
      "Score:  83.0\n",
      "Score:  74.0\n",
      "Score:  86.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  53.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  89.0\n",
      "Score:  75.0\n",
      "Score:  79.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  89.0\n",
      "Score:  53.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  65.0\n",
      "Score:  77.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  84.0\n",
      "Score:  85.0\n",
      "Score:  77.0\n",
      "Score:  92.0\n",
      "Score:  82.0\n",
      "Score:  83.0\n",
      "Score:  84.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  53.0\n",
      "Score:  73.0\n",
      "Score:  90.0\n",
      "Score:  67.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  87.0\n",
      "Score:  83.0\n",
      "Score:  53.0\n",
      "Score:  95.0\n",
      "Score:  53.0\n",
      "Score:  93.0\n",
      "Score:  94.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  91.0\n",
      "Score:  86.0\n",
      "Score:  91.0\n",
      "Score:  89.0\n",
      "Score:  87.0\n",
      "Score:  72.0\n",
      "Score:  67.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  75.0\n",
      "Score:  57.0\n",
      "Score:  57.0\n",
      "Score:  35.0\n",
      "Score:  45.0\n",
      "Score:  81.0\n",
      "Score:  81.0\n",
      "Score:  69.0\n",
      "Score:  80.0\n",
      "Score:  73.0\n",
      "Score:  68.0\n",
      "Score:  73.0\n",
      "Score:  91.0\n",
      "Score:  67.0\n",
      "Score:  54.0\n",
      "Score:  75.0\n",
      "Score:  54.0\n",
      "Score:  93.0\n",
      "Score:  69.0\n",
      "Score:  82.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  92.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  91.0\n",
      "Score:  95.0\n",
      "Score:  67.0\n",
      "Score:  95.0\n",
      "Score:  91.0\n",
      "Score:  95.0\n",
      "Score:  56.0\n",
      "Score:  35.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  83.0\n",
      "Score:  85.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  75.0\n",
      "Score:  73.0\n",
      "Score:  81.0\n",
      "Score:  69.0\n",
      "Score:  83.0\n",
      "Score:  73.0\n",
      "Score:  81.0\n",
      "Score:  67.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  80.0\n",
      "Score:  79.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  53.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  68.0\n",
      "Score:  53.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  71.0\n",
      "Score:  85.0\n",
      "Score:  81.0\n",
      "Score:  71.0\n",
      "Score:  75.0\n",
      "Score:  81.0\n",
      "Score:  93.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  74.0\n",
      "Score:  84.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  92.0\n",
      "Score:  74.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  83.0\n",
      "Score:  57.0\n",
      "Score:  84.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  66.0\n",
      "Score:  95.0\n",
      "Score:  75.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  80.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  92.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  73.0\n",
      "Score:  89.0\n",
      "Score:  80.0\n",
      "Score:  74.0\n",
      "Score:  54.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  79.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  16.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  92.0\n",
      "Score:  54.0\n",
      "Score:  76.0\n",
      "Score:  80.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  83.0\n",
      "Score:  16.0\n",
      "Score:  95.0\n",
      "Score:  94.0\n",
      "Score:  84.0\n",
      "Score:  84.0\n",
      "Score:  35.0\n",
      "Score:  57.0\n",
      "Score:  86.0\n",
      "Score:  86.0\n",
      "Score:  64.0\n",
      "Score:  54.0\n",
      "Score:  68.0\n",
      "Score:  80.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  94.0\n",
      "Score:  52.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  77.0\n",
      "Score:  94.0\n",
      "Score:  95.0\n",
      "Score:  92.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  95.0\n",
      "Score:  93.0\n",
      "Score:  57.0\n",
      "Score:  69.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  86.0\n",
      "Score:  91.0\n",
      "Score:  94.0\n",
      "Score:  85.0\n",
      "Score:  54.0\n",
      "Score:  85.0\n",
      "Score:  58.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  93.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  80.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  82.0\n",
      "Score:  35.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  58.0\n",
      "Score:  67.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  83.0\n",
      "Score:  86.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  88.0\n",
      "Score:  93.0\n",
      "Score:  95.0\n",
      "Score:  84.0\n",
      "Score:  82.0\n",
      "Score:  88.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  -5.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  80.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  57.0\n",
      "Score:  94.0\n",
      "Score:  55.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  88.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  80.0\n",
      "Score:  73.0\n",
      "Score:  91.0\n",
      "Score:  65.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  74.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  94.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  68.0\n",
      "Score:  76.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  80.0\n",
      "Score:  81.0\n",
      "Score:  73.0\n",
      "Score:  86.0\n",
      "Score:  92.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  75.0\n",
      "Score:  92.0\n",
      "Score:  84.0\n",
      "Score:  77.0\n",
      "Score:  89.0\n",
      "Score:  53.0\n",
      "Score:  79.0\n",
      "Score:  88.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  79.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  85.0\n",
      "Score:  83.0\n",
      "Score:  67.0\n",
      "Score:  91.0\n",
      "Score:  82.0\n",
      "Score:  82.0\n",
      "Score:  83.0\n",
      "Score:  86.0\n",
      "Score:  73.0\n",
      "Score:  57.0\n",
      "Score:  82.0\n",
      "Score:  83.0\n",
      "Score:  80.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  82.0\n",
      "Score:  92.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  35.0\n",
      "Score:  83.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  83.0\n",
      "Score:  79.0\n",
      "Score:  54.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  74.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  91.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  57.0\n",
      "Score:  69.0\n",
      "Score:  16.0\n",
      "Score:  88.0\n",
      "Score:  67.0\n",
      "Score:  80.0\n",
      "Score:  91.0\n",
      "Score:  73.0\n",
      "Score:  81.0\n",
      "Score:  54.0\n",
      "Score:  91.0\n",
      "Score:  81.0\n",
      "Score:  74.0\n",
      "Score:  69.0\n",
      "Score:  57.0\n",
      "Score:  17.0\n",
      "Score:  77.0\n",
      "Score:  16.0\n",
      "Score:  88.0\n",
      "Score:  57.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  68.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  86.0\n",
      "Score:  72.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  84.0\n",
      "Score:  73.0\n",
      "Score:  82.0\n",
      "Score:  82.0\n",
      "Score:  53.0\n",
      "Score:  36.0\n",
      "Score:  95.0\n",
      "Score:  57.0\n",
      "Score:  83.0\n",
      "Score:  79.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  85.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  68.0\n",
      "Score:  80.0\n",
      "Score:  86.0\n",
      "Score:  31.0\n",
      "Score:  82.0\n",
      "Score:  80.0\n",
      "Score:  88.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  80.0\n",
      "Score:  77.0\n",
      "Score:  81.0\n",
      "Score:  73.0\n",
      "Score:  86.0\n",
      "Score:  86.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  82.0\n",
      "Score:  57.0\n",
      "Score:  74.0\n",
      "Score:  90.0\n",
      "Score:  57.0\n",
      "Score:  57.0\n",
      "Score:  93.0\n",
      "Score:  84.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  79.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  81.0\n",
      "Score:  89.0\n",
      "Score:  77.0\n",
      "Score:  89.0\n",
      "Score:  77.0\n",
      "Score:  88.0\n",
      "Score:  68.0\n",
      "Score:  77.0\n",
      "Score:  90.0\n",
      "Score:  83.0\n",
      "Score:  35.0\n",
      "Score:  93.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  69.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  65.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  94.0\n",
      "Score:  91.0\n",
      "Score:  83.0\n",
      "Score:  72.0\n",
      "Score:  89.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  93.0\n",
      "Score:  67.0\n",
      "Score:  81.0\n",
      "Score:  57.0\n",
      "Score:  73.0\n",
      "Score:  78.0\n",
      "Score:  77.0\n",
      "Score:  80.0\n",
      "Score:  87.0\n",
      "Score:  75.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  89.0\n",
      "Score:  83.0\n",
      "Score:  93.0\n",
      "Score:  85.0\n",
      "Score:  54.0\n",
      "Score:  84.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  68.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  83.0\n",
      "Score:  57.0\n",
      "Score:  54.0\n",
      "Score:  74.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  69.0\n",
      "Score:  35.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  80.0\n",
      "Score:  68.0\n",
      "Score:  79.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  94.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  72.0\n",
      "Score:  81.0\n",
      "Score:  84.0\n",
      "Score:  76.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  91.0\n",
      "Score:  65.0\n",
      "Score:  72.0\n",
      "Score:  95.0\n",
      "Score:  91.0\n",
      "Score:  92.0\n",
      "Score:  75.0\n",
      "Score:  95.0\n",
      "Score:  75.0\n",
      "Score:  84.0\n",
      "Score:  33.0\n",
      "Score:  95.0\n",
      "Score:  91.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  73.0\n",
      "Score:  66.0\n",
      "Score:  57.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  86.0\n",
      "Score:  83.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  16.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  70.0\n",
      "Score:  80.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  79.0\n",
      "Score:  95.0\n",
      "Score:  69.0\n",
      "Score:  84.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  92.0\n",
      "Score:  81.0\n",
      "Score:  35.0\n",
      "Score:  88.0\n",
      "Score:  73.0\n",
      "Score:  80.0\n",
      "Score:  64.0\n",
      "Score:  77.0\n",
      "Score:  85.0\n",
      "Score:  68.0\n",
      "Score:  82.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  84.0\n",
      "Score:  69.0\n",
      "Score:  80.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  92.0\n",
      "Score:  92.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  87.0\n",
      "Score:  72.0\n",
      "Score:  54.0\n",
      "Score:  49.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  91.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  16.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  87.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  89.0\n",
      "Score:  95.0\n",
      "Score:  35.0\n",
      "Score:  81.0\n",
      "Score:  80.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  74.0\n",
      "Score:  72.0\n",
      "Score:  82.0\n",
      "Score:  82.0\n",
      "Score:  77.0\n",
      "Score:  74.0\n",
      "Score:  73.0\n",
      "Score:  88.0\n",
      "Score:  95.0\n",
      "Score:  58.0\n",
      "Score:  85.0\n",
      "Score:  91.0\n",
      "Score:  84.0\n",
      "Score:  67.0\n",
      "Score:  74.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  67.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  73.0\n",
      "Score:  84.0\n",
      "Score:  74.0\n",
      "Score:  83.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  94.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  82.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  35.0\n",
      "Score:  86.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  84.0\n",
      "Score:  82.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  74.0\n",
      "Score:  79.0\n",
      "Score:  93.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  75.0\n",
      "Score:  80.0\n",
      "Score:  73.0\n",
      "Score:  57.0\n",
      "Score:  80.0\n",
      "Score:  83.0\n",
      "Score:  45.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  67.0\n",
      "Score:  50.0\n",
      "Score:  54.0\n",
      "Score:  35.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  84.0\n",
      "Score:  87.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  57.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  58.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  35.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  84.0\n",
      "Score:  73.0\n",
      "Score:  87.0\n",
      "Score:  54.0\n",
      "Score:  85.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  55.0\n",
      "Score:  84.0\n",
      "Score:  16.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  89.0\n",
      "Score:  84.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  68.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  58.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  84.0\n",
      "Score:  80.0\n",
      "Score:  77.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  85.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  35.0\n",
      "Score:  85.0\n",
      "Score:  16.0\n",
      "Score:  82.0\n",
      "Score:  70.0\n",
      "Score:  77.0\n",
      "Score:  69.0\n",
      "Score:  57.0\n",
      "Score:  75.0\n",
      "Score:  95.0\n",
      "Score:  74.0\n",
      "Score:  79.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  16.0\n",
      "Score:  83.0\n",
      "Score:  80.0\n",
      "Score:  58.0\n",
      "Score:  77.0\n",
      "Score:  92.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  79.0\n",
      "Score:  87.0\n",
      "Score:  87.0\n",
      "Score:  78.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  72.0\n",
      "Score:  84.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  83.0\n",
      "Score:  91.0\n",
      "Score:  58.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  75.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  89.0\n",
      "Score:  80.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  74.0\n",
      "Score:  70.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  84.0\n",
      "Score:  95.0\n",
      "Score:  72.0\n",
      "Score:  35.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  82.0\n",
      "Score:  82.0\n",
      "Score:  81.0\n",
      "Score:  86.0\n",
      "Score:  92.0\n",
      "Score:  39.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  94.0\n",
      "Score:  33.0\n",
      "Score:  80.0\n",
      "Score:  83.0\n",
      "Score:  -4.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  58.0\n",
      "Score:  83.0\n",
      "Score:  93.0\n",
      "Score:  80.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  68.0\n",
      "Score:  67.0\n",
      "Score:  84.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  65.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  88.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  35.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  79.0\n",
      "Score:  91.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  93.0\n",
      "Score:  80.0\n",
      "Score:  70.0\n",
      "Score:  58.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  74.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  82.0\n",
      "Score:  58.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  91.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  80.0\n",
      "Score:  83.0\n",
      "Score:  35.0\n",
      "Score:  86.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  75.0\n",
      "Score:  81.0\n",
      "Score:  80.0\n",
      "Score:  95.0\n",
      "Score:  78.0\n",
      "Score:  94.0\n",
      "Score:  57.0\n",
      "Score:  92.0\n",
      "Score:  82.0\n",
      "Score:  77.0\n",
      "Score:  35.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  91.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  84.0\n",
      "Score:  80.0\n",
      "Score:  70.0\n",
      "Score:  80.0\n",
      "Score:  81.0\n",
      "Score:  54.0\n",
      "Score:  66.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  92.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  93.0\n",
      "Score:  54.0\n",
      "Score:  57.0\n",
      "Score:  84.0\n",
      "Score:  90.0\n",
      "Score:  83.0\n",
      "Score:  86.0\n",
      "Score:  53.0\n",
      "Score:  35.0\n",
      "Score:  74.0\n",
      "Score:  73.0\n",
      "Score:  92.0\n",
      "Score:  73.0\n",
      "Score:  94.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  86.0\n",
      "Score:  77.0\n",
      "Score:  92.0\n",
      "Score:  74.0\n",
      "Score:  88.0\n",
      "Score:  35.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  84.0\n",
      "Score:  69.0\n",
      "Score:  54.0\n",
      "Score:  75.0\n",
      "Score:  86.0\n",
      "Score:  94.0\n",
      "Score:  91.0\n",
      "Score:  95.0\n",
      "Score:  92.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  81.0\n",
      "Score:  69.0\n",
      "Score:  95.0\n",
      "Score:  94.0\n",
      "Score:  95.0\n",
      "Score:  92.0\n",
      "Score:  80.0\n",
      "Score:  95.0\n",
      "Score:  55.0\n",
      "Score:  70.0\n",
      "Score:  54.0\n",
      "Score:  85.0\n",
      "Score:  57.0\n",
      "Score:  94.0\n",
      "Score:  88.0\n",
      "Score:  84.0\n",
      "Score:  73.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  74.0\n",
      "Score:  87.0\n",
      "Score:  82.0\n",
      "Score:  75.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  81.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  94.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  80.0\n",
      "Score:  93.0\n",
      "Score:  73.0\n",
      "Score:  65.0\n",
      "Score:  93.0\n",
      "Score:  84.0\n",
      "Score:  57.0\n",
      "Score:  81.0\n",
      "Score:  89.0\n",
      "Score:  91.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  86.0\n",
      "Score:  84.0\n",
      "Score:  73.0\n",
      "Score:  68.0\n",
      "Score:  70.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  57.0\n",
      "Score:  77.0\n",
      "Score:  57.0\n",
      "Score:  94.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  74.0\n",
      "Score:  93.0\n",
      "Score:  75.0\n",
      "Score:  67.0\n",
      "Score:  81.0\n",
      "Score:  53.0\n",
      "Score:  95.0\n",
      "Score:  92.0\n",
      "Score:  77.0\n",
      "Score:  94.0\n",
      "Score:  69.0\n",
      "Score:  81.0\n",
      "Score:  74.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  67.0\n",
      "Score:  58.0\n",
      "Score:  77.0\n",
      "Score:  86.0\n",
      "Score:  66.0\n",
      "Score:  83.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  35.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  58.0\n",
      "Score:  72.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  16.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  89.0\n",
      "Score:  85.0\n",
      "Score:  81.0\n",
      "Score:  93.0\n",
      "Score:  73.0\n",
      "Score:  69.0\n",
      "Score:  57.0\n",
      "Score:  86.0\n",
      "Score:  86.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  57.0\n",
      "Score:  58.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  91.0\n",
      "Score:  83.0\n",
      "Score:  89.0\n",
      "Score:  74.0\n",
      "Score:  72.0\n",
      "Score:  81.0\n",
      "Score:  89.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  92.0\n",
      "Score:  83.0\n",
      "Score:  65.0\n",
      "Score:  80.0\n",
      "Score:  83.0\n",
      "Score:  35.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  83.0\n",
      "Score:  81.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  67.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  53.0\n",
      "Score:  74.0\n",
      "Score:  77.0\n",
      "Score:  74.0\n",
      "Score:  83.0\n",
      "Score:  92.0\n",
      "Score:  54.0\n",
      "Score:  53.0\n",
      "Score:  85.0\n",
      "Score:  77.0\n",
      "Score:  58.0\n",
      "Score:  85.0\n",
      "Score:  83.0\n",
      "Score:  83.0\n",
      "Score:  91.0\n",
      "Score:  73.0\n",
      "Score:  92.0\n",
      "Score:  74.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  79.0\n",
      "Score:  65.0\n",
      "Score:  85.0\n",
      "Score:  11.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  85.0\n",
      "Score:  -355.0\n",
      "Score:  81.0\n",
      "Score:  73.0\n",
      "Score:  74.0\n",
      "Score:  77.0\n",
      "Score:  89.0\n",
      "Score:  86.0\n",
      "Score:  33.0\n",
      "Score:  77.0\n",
      "Score:  67.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  82.0\n",
      "Score:  53.0\n",
      "Score:  85.0\n",
      "Score:  16.0\n",
      "Score:  84.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  57.0\n",
      "Score:  54.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  93.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  94.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  91.0\n",
      "Score:  68.0\n",
      "Score:  57.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  94.0\n",
      "Score:  95.0\n",
      "Score:  50.0\n",
      "Score:  54.0\n",
      "Score:  84.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  57.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  74.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  80.0\n",
      "Score:  95.0\n",
      "Score:  58.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  85.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  92.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  84.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  69.0\n",
      "Score:  82.0\n",
      "Score:  68.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  82.0\n",
      "Score:  77.0\n",
      "Score:  85.0\n",
      "Score:  79.0\n",
      "Score:  69.0\n",
      "Score:  -46.0\n",
      "Score:  93.0\n",
      "Score:  73.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  69.0\n",
      "Score:  85.0\n",
      "Score:  35.0\n",
      "Score:  73.0\n",
      "Score:  84.0\n",
      "Score:  92.0\n",
      "Score:  84.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  91.0\n",
      "Score:  86.0\n",
      "Score:  83.0\n",
      "Score:  54.0\n",
      "Score:  65.0\n",
      "Score:  85.0\n",
      "Score:  48.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  92.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  93.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  67.0\n",
      "Score:  77.0\n",
      "Score:  81.0\n",
      "Score:  85.0\n",
      "Score:  92.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  87.0\n",
      "Score:  81.0\n",
      "Score:  94.0\n",
      "Score:  94.0\n",
      "Score:  53.0\n",
      "Score:  83.0\n",
      "Score:  57.0\n",
      "Score:  81.0\n",
      "Score:  93.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  74.0\n",
      "Score:  77.0\n",
      "Score:  93.0\n",
      "Score:  82.0\n",
      "Score:  67.0\n",
      "Score:  16.0\n",
      "Score:  74.0\n",
      "Score:  66.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  68.0\n",
      "Score:  79.0\n",
      "Score:  84.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  78.0\n",
      "Score:  77.0\n",
      "Score:  35.0\n",
      "Score:  83.0\n",
      "Score:  73.0\n",
      "Score:  87.0\n",
      "Score:  53.0\n",
      "Score:  77.0\n",
      "Score:  86.0\n",
      "Score:  77.0\n",
      "Score:  92.0\n",
      "Score:  85.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  70.0\n",
      "Score:  85.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  89.0\n",
      "Score:  77.0\n",
      "Score:  50.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  95.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  84.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  80.0\n",
      "Score:  67.0\n",
      "Score:  75.0\n",
      "Score:  87.0\n",
      "Score:  85.0\n",
      "Score:  82.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  92.0\n",
      "Score:  95.0\n",
      "Score:  80.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  57.0\n",
      "Score:  91.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  80.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  82.0\n",
      "Score:  57.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  86.0\n",
      "Score:  81.0\n",
      "Score:  53.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  67.0\n",
      "Score:  83.0\n",
      "Score:  73.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  82.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  91.0\n",
      "Score:  83.0\n",
      "Score:  74.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  92.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  87.0\n",
      "Score:  83.0\n",
      "Score:  81.0\n",
      "Score:  74.0\n",
      "Score:  92.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  84.0\n",
      "Score:  58.0\n",
      "Score:  95.0\n",
      "Score:  94.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  83.0\n",
      "Score:  53.0\n",
      "Score:  85.0\n",
      "Score:  57.0\n",
      "Score:  74.0\n",
      "Score:  74.0\n",
      "Score:  73.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  54.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  57.0\n",
      "Score:  57.0\n",
      "Score:  82.0\n",
      "Score:  79.0\n",
      "Score:  95.0\n",
      "Score:  93.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  68.0\n",
      "Score:  83.0\n",
      "Score:  73.0\n",
      "Score:  73.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  81.0\n",
      "Score:  35.0\n",
      "Score:  64.0\n",
      "Score:  74.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  91.0\n",
      "Score:  75.0\n",
      "Score:  57.0\n",
      "Score:  73.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  69.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  49.0\n",
      "Score:  82.0\n",
      "Score:  39.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  87.0\n",
      "Score:  67.0\n",
      "Score:  73.0\n",
      "Score:  68.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  80.0\n",
      "Score:  85.0\n",
      "Score:  73.0\n",
      "Score:  58.0\n",
      "Score:  78.0\n",
      "Score:  81.0\n",
      "Score:  86.0\n",
      "Score:  81.0\n",
      "Score:  85.0\n",
      "Score:  82.0\n",
      "Score:  85.0\n",
      "Score:  93.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  86.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  72.0\n",
      "Score:  95.0\n",
      "Score:  77.0\n",
      "Score:  67.0\n",
      "Score:  70.0\n",
      "Score:  87.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  87.0\n",
      "Score:  88.0\n",
      "Score:  65.0\n",
      "Score:  36.0\n",
      "Score:  77.0\n",
      "Score:  73.0\n",
      "Score:  53.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  74.0\n",
      "Score:  92.0\n",
      "Score:  80.0\n",
      "Score:  83.0\n",
      "Score:  75.0\n",
      "Score:  58.0\n",
      "Score:  16.0\n",
      "Score:  39.0\n",
      "Score:  81.0\n",
      "Score:  73.0\n",
      "Score:  93.0\n",
      "Score:  93.0\n",
      "Score:  77.0\n",
      "Score:  67.0\n",
      "Score:  80.0\n",
      "Score:  65.0\n",
      "Score:  36.0\n",
      "Score:  80.0\n",
      "Score:  89.0\n",
      "Score:  65.0\n",
      "Score:  80.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  93.0\n",
      "Score:  54.0\n",
      "Score:  73.0\n",
      "Score:  86.0\n",
      "Score:  67.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  73.0\n",
      "Score:  77.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n",
      "Score:  70.0\n",
      "Score:  77.0\n",
      "Score:  92.0\n",
      "Score:  83.0\n",
      "Score:  77.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  68.0\n",
      "Score:  67.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  84.0\n",
      "Score:  57.0\n",
      "Score:  84.0\n",
      "Score:  87.0\n",
      "Score:  77.0\n",
      "Score:  16.0\n",
      "Score:  84.0\n",
      "Score:  78.0\n",
      "Score:  73.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  83.0\n",
      "Score:  81.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  91.0\n",
      "Score:  83.0\n",
      "Score:  81.0\n",
      "Score:  67.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  16.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  84.0\n",
      "Score:  95.0\n",
      "Score:  79.0\n",
      "Score:  85.0\n",
      "Score:  68.0\n",
      "Score:  95.0\n",
      "Score:  82.0\n",
      "Score:  79.0\n",
      "Score:  77.0\n",
      "Score:  69.0\n",
      "Score:  58.0\n",
      "Score:  82.0\n",
      "Score:  77.0\n",
      "Score:  92.0\n",
      "Score:  83.0\n",
      "Score:  35.0\n",
      "Score:  77.0\n",
      "Score:  79.0\n",
      "Score:  36.0\n",
      "Score:  92.0\n",
      "Score:  70.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n",
      "Score:  80.0\n",
      "Score:  93.0\n",
      "Score:  82.0\n",
      "Score:  67.0\n",
      "Score:  73.0\n",
      "Score:  50.0\n",
      "Score:  84.0\n",
      "Score:  83.0\n",
      "Score:  95.0\n",
      "Score:  79.0\n",
      "Score:  85.0\n",
      "Score:  54.0\n",
      "Score:  54.0\n",
      "Score:  81.0\n",
      "Score:  54.0\n",
      "Score:  16.0\n",
      "Score:  95.0\n",
      "Score:  83.0\n",
      "Score:  88.0\n",
      "Score:  82.0\n",
      "Score:  54.0\n",
      "Score:  70.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  70.0\n",
      "Score:  95.0\n",
      "Score:  67.0\n",
      "Score:  86.0\n",
      "Score:  87.0\n",
      "Score:  77.0\n",
      "Score:  82.0\n",
      "Score:  74.0\n",
      "Score:  16.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -4.0\n",
      "Score:  58.0\n",
      "Score:  73.0\n",
      "Score:  82.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  85.0\n",
      "Score:  80.0\n",
      "Score:  77.0\n",
      "Score:  77.0\n",
      "Score:  58.0\n",
      "Score:  95.0\n",
      "Score:  72.0\n",
      "Score:  54.0\n",
      "Score:  86.0\n",
      "Score:  73.0\n",
      "Score:  81.0\n",
      "Score:  75.0\n",
      "Score:  35.0\n",
      "Score:  73.0\n",
      "Score:  54.0\n",
      "Score:  95.0\n",
      "Score:  54.0\n",
      "Score:  81.0\n",
      "Score:  83.0\n",
      "Score:  82.0\n"
     ]
    },
    {
     "ename": "SignalException",
     "evalue": "Signal SIGINT received. ViZDoom instance has been closed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSignalException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-53ea9b5bd0bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpossible_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_total_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSignalException\u001b[0m: Signal SIGINT received. ViZDoom instance has been closed."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = create_enviroment()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(100000):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                #print(\"else\")\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
